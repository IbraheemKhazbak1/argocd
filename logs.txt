
==> Audit <==
|--------------|--------------------------------|----------|-----------------------------|---------|----------------------|----------------------|
|   Command    |              Args              | Profile  |            User             | Version |      Start Time      |       End Time       |
|--------------|--------------------------------|----------|-----------------------------|---------|----------------------|----------------------|
| start        | --driver docker                | minikube | programming                 | v1.35.0 | 16 Apr 25 12:03 EET  | 16 Apr 25 12:04 EET  |
| start        | --driver docker                | minikube | programming                 | v1.35.0 | 22 Apr 25 09:47 EET  | 22 Apr 25 09:47 EET  |
| start        | --driver docker                | minikube | programming                 | v1.35.0 | 25 Apr 25 08:10 EEST | 25 Apr 25 08:11 EEST |
| start        |                                | minikube | programming                 | v1.35.0 | 27 Apr 25 09:28 EEST |                      |
| start        | --driver docker                | minikube | programming                 | v1.35.0 | 27 Apr 25 09:28 EEST |                      |
| start        | --driver docker                | minikube | programming                 | v1.35.0 | 27 Apr 25 09:33 EEST |                      |
| start        |                                | minikube | programming                 | v1.35.0 | 27 Apr 25 09:34 EEST |                      |
| start        |                                | minikube | programming                 | v1.35.0 | 27 Apr 25 09:34 EEST |                      |
| start        | --driver docker                | minikube | programming                 | v1.35.0 | 27 Apr 25 09:34 EEST |                      |
| delete       |                                | minikube | programming                 | v1.35.0 | 27 Apr 25 09:35 EEST | 27 Apr 25 09:36 EEST |
| start        | --driver docker                | minikube | programming                 | v1.35.0 | 27 Apr 25 09:36 EEST |                      |
| start        | --driver docker                | minikube | programming                 | v1.35.0 | 27 Apr 25 09:39 EEST |                      |
| start        |                                | minikube | programming                 | v1.35.0 | 27 Apr 25 09:39 EEST |                      |
| start        |                                | minikube | programming                 | v1.35.0 | 27 Apr 25 11:04 EEST |                      |
| start        | --driver=docker                | minikube | programming                 | v1.35.0 | 27 Apr 25 11:04 EEST |                      |
| start        | --driver=docker                | minikube | programming                 | v1.35.0 | 27 Apr 25 11:04 EEST | 27 Apr 25 11:05 EEST |
| start        | --driver=docker                | minikube | programming                 | v1.35.0 | 27 Apr 25 11:05 EEST | 27 Apr 25 11:05 EEST |
| start        | --driver docker                | minikube | programming                 | v1.35.0 | 28 Apr 25 09:16 EEST | 28 Apr 25 09:17 EEST |
| docker-env   |                                | minikube | programming                 | v1.35.0 | 28 Apr 25 10:16 EEST | 28 Apr 25 10:16 EEST |
| dashboard    |                                | minikube | programming                 | v1.35.0 | 28 Apr 25 10:59 EEST |                      |
| dashboard    |                                | minikube | programming                 | v1.35.0 | 28 Apr 25 11:24 EEST |                      |
| docker-env   |                                | minikube | programming                 | v1.35.0 | 28 Apr 25 12:05 EEST | 28 Apr 25 12:05 EEST |
| start        |                                | minikube | programming                 | v1.35.0 | 29 Apr 25 09:33 EEST | 29 Apr 25 09:34 EEST |
| update-check |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 05 May 25 10:25 EEST | 05 May 25 10:25 EEST |
| update-check |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 27 May 25 12:23 EEST | 27 May 25 12:23 EEST |
| start        |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 25 Jul 25 18:23 EEST |                      |
| start        |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 25 Jul 25 18:25 EEST |                      |
| update-check |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 07 Aug 25 11:09 EEST | 07 Aug 25 11:09 EEST |
| update-check |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 23 Aug 25 17:06 EEST | 23 Aug 25 17:06 EEST |
| update-check |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 23 Aug 25 17:33 EEST | 23 Aug 25 17:33 EEST |
| update-check |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 24 Aug 25 17:37 EEST | 24 Aug 25 17:37 EEST |
| update-check |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 24 Aug 25 17:43 EEST | 24 Aug 25 17:44 EEST |
| update-check |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 25 Aug 25 17:47 EEST | 25 Aug 25 17:47 EEST |
| update-check |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 07 Sep 25 10:54 EEST | 07 Sep 25 10:54 EEST |
| update-check |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 08 Sep 25 16:25 EEST | 08 Sep 25 16:25 EEST |
| update-check |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 21 Sep 25 08:08 EEST | 21 Sep 25 08:08 EEST |
| update-check |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 21 Sep 25 10:03 EEST | 21 Sep 25 10:03 EEST |
| update-check |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 21 Sep 25 10:41 EEST | 21 Sep 25 10:41 EEST |
| update-check |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 21 Sep 25 11:20 EEST | 21 Sep 25 11:20 EEST |
| update-check |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 21 Sep 25 11:48 EEST | 21 Sep 25 11:48 EEST |
| update-check |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 22 Sep 25 09:44 EEST | 22 Sep 25 09:44 EEST |
| update-check |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 25 Sep 25 16:48 EEST | 25 Sep 25 16:48 EEST |
| update-check |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 28 Sep 25 10:46 EEST | 28 Sep 25 10:46 EEST |
| update-check |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 28 Sep 25 10:47 EEST | 28 Sep 25 10:47 EEST |
| update-check |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 29 Sep 25 14:02 EEST | 29 Sep 25 14:02 EEST |
| update-check |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 30 Sep 25 10:53 EEST | 30 Sep 25 10:53 EEST |
| update-check |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 01 Oct 25 15:09 EEST | 01 Oct 25 15:09 EEST |
| start        |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 18 Jan 26 11:50 EET  | 18 Jan 26 11:52 EET  |
| docker-env   | minikube docker-env            | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 18 Jan 26 12:01 EET  | 18 Jan 26 12:01 EET  |
| stop         |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 18 Jan 26 12:09 EET  | 18 Jan 26 12:09 EET  |
| start        |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 18 Jan 26 12:10 EET  | 18 Jan 26 12:10 EET  |
| docker-env   |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 18 Jan 26 15:41 EET  | 18 Jan 26 15:41 EET  |
| docker-env   |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 18 Jan 26 16:47 EET  | 18 Jan 26 16:47 EET  |
| docker-env   | minikube docker-env --shell    | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 18 Jan 26 16:47 EET  | 18 Jan 26 16:47 EET  |
|              | powershell                     |          |                             |         |                      |                      |
| docker-env   |                                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 18 Jan 26 16:48 EET  | 18 Jan 26 16:48 EET  |
| docker-env   | minikube docker-env --shell    | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 18 Jan 26 16:49 EET  | 18 Jan 26 16:49 EET  |
|              | powershell                     |          |                             |         |                      |                      |
| image        | load weatherapi:latest         | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 18 Jan 26 16:51 EET  |                      |
| image        | load weatherapi:latest         | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 18 Jan 26 16:52 EET  |                      |
| image        | load                           | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 18 Jan 26 16:53 EET  |                      |
| image        | load weatherapi                | minikube | DESKTOP-IGML589\Programming | v1.35.0 | 18 Jan 26 16:53 EET  |                      |
|--------------|--------------------------------|----------|-----------------------------|---------|----------------------|----------------------|


==> Last Start <==
Log file created at: 2026/01/18 12:10:02
Running on machine: DESKTOP-IGML589
Binary: Built with gc go1.23.4 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0118 12:10:02.883886   20212 out.go:345] Setting OutFile to fd 108 ...
I0118 12:10:02.884890   20212 out.go:392] TERM=,COLORTERM=, which probably does not support color
I0118 12:10:02.884890   20212 out.go:358] Setting ErrFile to fd 112...
I0118 12:10:02.884890   20212 out.go:392] TERM=,COLORTERM=, which probably does not support color
I0118 12:10:02.884890   20212 oci.go:582] shell is pointing to dockerd inside minikube. will unset to use host
I0118 12:10:02.904951   20212 out.go:352] Setting JSON to false
I0118 12:10:02.905464   20212 start.go:129] hostinfo: {"hostname":"DESKTOP-IGML589","uptime":98444,"bootTime":1768632558,"procs":377,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.26200.7462 Build 26200.7462","kernelVersion":"10.0.26200.7462 Build 26200.7462","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"b0300280-c2ed-45df-b5f6-2f8e55a7aa20"}
W0118 12:10:02.905464   20212 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0118 12:10:02.907142   20212 out.go:177] * minikube v1.35.0 on Microsoft Windows 11 Pro 10.0.26200.7462 Build 26200.7462
I0118 12:10:02.908174   20212 notify.go:220] Checking for updates...
I0118 12:10:02.908687   20212 out.go:177]   - KUBECONFIG="C:\Users\Programming\.kube\configLocal"
I0118 12:10:02.910221   20212 out.go:177]   - MINIKUBE_ACTIVE_DOCKERD=minikube
I0118 12:10:02.911817   20212 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0118 12:10:02.911817   20212 driver.go:394] Setting default libvirt URI to qemu:///system
I0118 12:10:02.974870   20212 docker.go:123] docker version: linux-29.0.1:Docker Desktop 4.52.0 (210994)
I0118 12:10:02.982107   20212 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0118 12:10:03.143672   20212 info.go:266] docker info: {ID:a3d51065-980a-4d0b-b0b6-136c896c0c25 Containers:3 ContainersRunning:0 ContainersPaused:0 ContainersStopped:3 Images:10 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:51 OomKillDisable:false NGoroutines:83 SystemTime:2026-01-18 10:10:02.964687218 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:16683036672 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:29.0.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:fcd43222d6b07379a4be9786bda52438f0dd16a1 Expected:} RuncCommit:{ID:v1.3.3-0-gd842d771 Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.3-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.45] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.28.0] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v1.0.0] map[Name:offload Path:C:\Program Files\Docker\cli-plugins\docker-offload.exe SchemaVersion:0.1.0 ShortDescription:Docker Offload Vendor:Docker Inc. Version:v0.5.20] map[Hidden:true Name:pass Path:C:\Program Files\Docker\cli-plugins\docker-pass.exe SchemaVersion:0.1.0 ShortDescription:Docker Pass Secrets Manager Plugin (beta) Vendor:Docker Inc. Version:v0.0.11] map[Name:sandbox Path:C:\Program Files\Docker\cli-plugins\docker-sandbox.exe SchemaVersion:0.1.0 ShortDescription:Docker Sandbox Vendor:Docker Inc. Version:v0.6.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I0118 12:10:03.144186   20212 out.go:177] * Using the docker driver based on existing profile
I0118 12:10:03.145225   20212 start.go:297] selected driver: docker
I0118 12:10:03.145225   20212 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/programming:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0118 12:10:03.145225   20212 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0118 12:10:03.158607   20212 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0118 12:10:03.330642   20212 info.go:266] docker info: {ID:a3d51065-980a-4d0b-b0b6-136c896c0c25 Containers:3 ContainersRunning:0 ContainersPaused:0 ContainersStopped:3 Images:10 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:51 OomKillDisable:false NGoroutines:83 SystemTime:2026-01-18 10:10:03.159975358 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:16683036672 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:29.0.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:fcd43222d6b07379a4be9786bda52438f0dd16a1 Expected:} RuncCommit:{ID:v1.3.3-0-gd842d771 Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.3-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.45] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.28.0] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v1.0.0] map[Name:offload Path:C:\Program Files\Docker\cli-plugins\docker-offload.exe SchemaVersion:0.1.0 ShortDescription:Docker Offload Vendor:Docker Inc. Version:v0.5.20] map[Hidden:true Name:pass Path:C:\Program Files\Docker\cli-plugins\docker-pass.exe SchemaVersion:0.1.0 ShortDescription:Docker Pass Secrets Manager Plugin (beta) Vendor:Docker Inc. Version:v0.0.11] map[Name:sandbox Path:C:\Program Files\Docker\cli-plugins\docker-sandbox.exe SchemaVersion:0.1.0 ShortDescription:Docker Sandbox Vendor:Docker Inc. Version:v0.6.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I0118 12:10:03.412155   20212 cni.go:84] Creating CNI manager for ""
I0118 12:10:03.412155   20212 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0118 12:10:03.412667   20212 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/programming:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0118 12:10:03.413182   20212 out.go:177] * Starting "minikube" primary control-plane node in "minikube" cluster
I0118 12:10:03.414208   20212 cache.go:121] Beginning downloading kic base image for docker with docker
I0118 12:10:03.414723   20212 out.go:177] * Pulling base image v0.0.46 ...
I0118 12:10:03.415743   20212 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0118 12:10:03.415743   20212 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0118 12:10:03.415743   20212 preload.go:146] Found local preload: C:\Users\Programming\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0118 12:10:03.416256   20212 cache.go:56] Caching tarball of preloaded images
I0118 12:10:03.416256   20212 preload.go:172] Found C:\Users\Programming\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0118 12:10:03.416256   20212 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0118 12:10:03.416256   20212 profile.go:143] Saving config to C:\Users\Programming\.minikube\profiles\minikube\config.json ...
I0118 12:10:03.473242   20212 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull
I0118 12:10:03.473242   20212 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load
I0118 12:10:03.473242   20212 cache.go:227] Successfully downloaded all kic artifacts
I0118 12:10:03.473765   20212 start.go:360] acquireMachinesLock for minikube: {Name:mka93ea91bca8798b37e9efc2b9a9ea5c4193692 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0118 12:10:03.473765   20212 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0118 12:10:03.473765   20212 start.go:96] Skipping create...Using existing machine configuration
I0118 12:10:03.473765   20212 fix.go:54] fixHost starting: 
I0118 12:10:03.474808   20212 out.go:177] * Noticed you have an activated docker-env on docker driver in this terminal:
W0118 12:10:03.475329   20212 out.go:270] ! Please re-eval your docker-env, To ensure your environment variables have updated ports:

	'minikube -p minikube docker-env'

	
I0118 12:10:03.489274   20212 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0118 12:10:03.536894   20212 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0118 12:10:03.536894   20212 fix.go:138] unexpected machine state, will restart: <nil>
I0118 12:10:03.537922   20212 out.go:177] * Restarting existing docker container for "minikube" ...
I0118 12:10:03.545647   20212 cli_runner.go:164] Run: docker start minikube
I0118 12:10:03.893405   20212 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0118 12:10:03.939050   20212 kic.go:430] container "minikube" state is running.
I0118 12:10:03.948475   20212 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0118 12:10:04.002293   20212 profile.go:143] Saving config to C:\Users\Programming\.minikube\profiles\minikube\config.json ...
I0118 12:10:04.003349   20212 machine.go:93] provisionDockerMachine start ...
I0118 12:10:04.011101   20212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0118 12:10:04.063609   20212 main.go:141] libmachine: Using SSH client type: native
I0118 12:10:04.064123   20212 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x985360] 0x987ea0 <nil>  [] 0s} 127.0.0.1 50731 <nil> <nil>}
I0118 12:10:04.064123   20212 main.go:141] libmachine: About to run SSH command:
hostname
I0118 12:10:04.065162   20212 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0118 12:10:07.190192   20212 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0118 12:10:07.190192   20212 ubuntu.go:169] provisioning hostname "minikube"
I0118 12:10:07.196872   20212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0118 12:10:07.246689   20212 main.go:141] libmachine: Using SSH client type: native
I0118 12:10:07.246689   20212 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x985360] 0x987ea0 <nil>  [] 0s} 127.0.0.1 50731 <nil> <nil>}
I0118 12:10:07.246689   20212 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0118 12:10:07.382403   20212 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0118 12:10:07.389777   20212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0118 12:10:07.439038   20212 main.go:141] libmachine: Using SSH client type: native
I0118 12:10:07.439554   20212 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x985360] 0x987ea0 <nil>  [] 0s} 127.0.0.1 50731 <nil> <nil>}
I0118 12:10:07.439554   20212 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0118 12:10:07.562225   20212 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0118 12:10:07.562225   20212 ubuntu.go:175] set auth options {CertDir:C:\Users\Programming\.minikube CaCertPath:C:\Users\Programming\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Programming\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Programming\.minikube\machines\server.pem ServerKeyPath:C:\Users\Programming\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Programming\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Programming\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Programming\.minikube}
I0118 12:10:07.562225   20212 ubuntu.go:177] setting up certificates
I0118 12:10:07.562225   20212 provision.go:84] configureAuth start
I0118 12:10:07.570072   20212 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0118 12:10:07.612528   20212 provision.go:143] copyHostCerts
I0118 12:10:07.612528   20212 exec_runner.go:144] found C:\Users\Programming\.minikube/ca.pem, removing ...
I0118 12:10:07.612528   20212 exec_runner.go:203] rm: C:\Users\Programming\.minikube\ca.pem
I0118 12:10:07.613105   20212 exec_runner.go:151] cp: C:\Users\Programming\.minikube\certs\ca.pem --> C:\Users\Programming\.minikube/ca.pem (1090 bytes)
I0118 12:10:07.613624   20212 exec_runner.go:144] found C:\Users\Programming\.minikube/cert.pem, removing ...
I0118 12:10:07.613624   20212 exec_runner.go:203] rm: C:\Users\Programming\.minikube\cert.pem
I0118 12:10:07.614145   20212 exec_runner.go:151] cp: C:\Users\Programming\.minikube\certs\cert.pem --> C:\Users\Programming\.minikube/cert.pem (1135 bytes)
I0118 12:10:07.614662   20212 exec_runner.go:144] found C:\Users\Programming\.minikube/key.pem, removing ...
I0118 12:10:07.614662   20212 exec_runner.go:203] rm: C:\Users\Programming\.minikube\key.pem
I0118 12:10:07.614662   20212 exec_runner.go:151] cp: C:\Users\Programming\.minikube\certs\key.pem --> C:\Users\Programming\.minikube/key.pem (1679 bytes)
I0118 12:10:07.615184   20212 provision.go:117] generating server cert: C:\Users\Programming\.minikube\machines\server.pem ca-key=C:\Users\Programming\.minikube\certs\ca.pem private-key=C:\Users\Programming\.minikube\certs\ca-key.pem org=Programming.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0118 12:10:07.729894   20212 provision.go:177] copyRemoteCerts
I0118 12:10:07.732897   20212 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0118 12:10:07.739523   20212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0118 12:10:07.785238   20212 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50731 SSHKeyPath:C:\Users\Programming\.minikube\machines\minikube\id_rsa Username:docker}
I0118 12:10:07.880465   20212 ssh_runner.go:362] scp C:\Users\Programming\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1090 bytes)
I0118 12:10:07.901717   20212 ssh_runner.go:362] scp C:\Users\Programming\.minikube\machines\server.pem --> /etc/docker/server.pem (1192 bytes)
I0118 12:10:07.920536   20212 ssh_runner.go:362] scp C:\Users\Programming\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0118 12:10:07.938414   20212 provision.go:87] duration metric: took 376.1885ms to configureAuth
I0118 12:10:07.938414   20212 ubuntu.go:193] setting minikube options for container-runtime
I0118 12:10:07.938924   20212 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0118 12:10:07.945617   20212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0118 12:10:07.996834   20212 main.go:141] libmachine: Using SSH client type: native
I0118 12:10:07.997347   20212 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x985360] 0x987ea0 <nil>  [] 0s} 127.0.0.1 50731 <nil> <nil>}
I0118 12:10:07.997347   20212 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0118 12:10:08.136898   20212 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0118 12:10:08.136898   20212 ubuntu.go:71] root file system type: overlay
I0118 12:10:08.136898   20212 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0118 12:10:08.148218   20212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0118 12:10:08.198917   20212 main.go:141] libmachine: Using SSH client type: native
I0118 12:10:08.198917   20212 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x985360] 0x987ea0 <nil>  [] 0s} 127.0.0.1 50731 <nil> <nil>}
I0118 12:10:08.198917   20212 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0118 12:10:08.326044   20212 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0118 12:10:08.333349   20212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0118 12:10:08.384843   20212 main.go:141] libmachine: Using SSH client type: native
I0118 12:10:08.385356   20212 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x985360] 0x987ea0 <nil>  [] 0s} 127.0.0.1 50731 <nil> <nil>}
I0118 12:10:08.385356   20212 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0118 12:10:08.517699   20212 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0118 12:10:08.517699   20212 machine.go:96] duration metric: took 4.5143499s to provisionDockerMachine
I0118 12:10:08.517699   20212 start.go:293] postStartSetup for "minikube" (driver="docker")
I0118 12:10:08.517699   20212 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0118 12:10:08.522361   20212 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0118 12:10:08.529065   20212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0118 12:10:08.574318   20212 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50731 SSHKeyPath:C:\Users\Programming\.minikube\machines\minikube\id_rsa Username:docker}
I0118 12:10:08.706759   20212 ssh_runner.go:195] Run: cat /etc/os-release
I0118 12:10:08.710462   20212 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0118 12:10:08.710982   20212 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0118 12:10:08.710982   20212 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0118 12:10:08.710982   20212 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0118 12:10:08.710982   20212 filesync.go:126] Scanning C:\Users\Programming\.minikube\addons for local assets ...
I0118 12:10:08.710982   20212 filesync.go:126] Scanning C:\Users\Programming\.minikube\files for local assets ...
I0118 12:10:08.710982   20212 start.go:296] duration metric: took 193.2834ms for postStartSetup
I0118 12:10:08.748521   20212 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0118 12:10:08.754208   20212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0118 12:10:08.797556   20212 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50731 SSHKeyPath:C:\Users\Programming\.minikube\machines\minikube\id_rsa Username:docker}
I0118 12:10:08.928671   20212 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0118 12:10:08.933670   20212 fix.go:56] duration metric: took 5.4599056s for fixHost
I0118 12:10:08.933670   20212 start.go:83] releasing machines lock for "minikube", held for 5.4599056s
I0118 12:10:08.942332   20212 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0118 12:10:08.992147   20212 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0118 12:10:08.999418   20212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0118 12:10:09.029490   20212 ssh_runner.go:195] Run: cat /version.json
I0118 12:10:09.036715   20212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0118 12:10:09.045157   20212 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50731 SSHKeyPath:C:\Users\Programming\.minikube\machines\minikube\id_rsa Username:docker}
I0118 12:10:09.080636   20212 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50731 SSHKeyPath:C:\Users\Programming\.minikube\machines\minikube\id_rsa Username:docker}
W0118 12:10:09.125791   20212 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0118 12:10:09.207700   20212 ssh_runner.go:195] Run: systemctl --version
I0118 12:10:09.251630   20212 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0118 12:10:09.260664   20212 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0118 12:10:09.270169   20212 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0118 12:10:09.274299   20212 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0118 12:10:09.282697   20212 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0118 12:10:09.282697   20212 start.go:495] detecting cgroup driver to use...
I0118 12:10:09.282697   20212 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0118 12:10:09.282697   20212 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0118 12:10:09.338313   20212 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0118 12:10:09.390299   20212 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
W0118 12:10:09.390299   20212 out.go:270] ! Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0118 12:10:09.390812   20212 out.go:270] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0118 12:10:09.400920   20212 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0118 12:10:09.441601   20212 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0118 12:10:09.494326   20212 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0118 12:10:09.546718   20212 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0118 12:10:09.596430   20212 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0118 12:10:09.650059   20212 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0118 12:10:09.702353   20212 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0118 12:10:09.755280   20212 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0118 12:10:09.808798   20212 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0118 12:10:09.823256   20212 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0118 12:10:09.835537   20212 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0118 12:10:09.848115   20212 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0118 12:10:09.907753   20212 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0118 12:10:10.012950   20212 start.go:495] detecting cgroup driver to use...
I0118 12:10:10.012950   20212 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0118 12:10:10.018242   20212 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0118 12:10:10.028702   20212 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0118 12:10:10.032892   20212 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0118 12:10:10.043062   20212 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0118 12:10:10.102725   20212 ssh_runner.go:195] Run: which cri-dockerd
I0118 12:10:10.109838   20212 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0118 12:10:10.117609   20212 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0118 12:10:10.134486   20212 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0118 12:10:10.197849   20212 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0118 12:10:10.257669   20212 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0118 12:10:10.257669   20212 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0118 12:10:10.275130   20212 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0118 12:10:10.333157   20212 ssh_runner.go:195] Run: sudo systemctl restart docker
I0118 12:10:12.320857   20212 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.9876994s)
I0118 12:10:12.325061   20212 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0118 12:10:12.339031   20212 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0118 12:10:12.353840   20212 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0118 12:10:12.367644   20212 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0118 12:10:12.425495   20212 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0118 12:10:12.483475   20212 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0118 12:10:12.538496   20212 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0118 12:10:12.580527   20212 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0118 12:10:12.594832   20212 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0118 12:10:12.656940   20212 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0118 12:10:12.746120   20212 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0118 12:10:12.787587   20212 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0118 12:10:12.791952   20212 start.go:563] Will wait 60s for crictl version
I0118 12:10:12.833414   20212 ssh_runner.go:195] Run: which crictl
I0118 12:10:12.840943   20212 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0118 12:10:12.929254   20212 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0118 12:10:12.936013   20212 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0118 12:10:12.964859   20212 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0118 12:10:12.986336   20212 out.go:235] * Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0118 12:10:12.993571   20212 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0118 12:10:13.128851   20212 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0118 12:10:13.169242   20212 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0118 12:10:13.173847   20212 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0118 12:10:13.190056   20212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0118 12:10:13.237426   20212 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/programming:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0118 12:10:13.237426   20212 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0118 12:10:13.245145   20212 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0118 12:10:13.265652   20212 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0118 12:10:13.265652   20212 docker.go:619] Images already preloaded, skipping extraction
I0118 12:10:13.272851   20212 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0118 12:10:13.292010   20212 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0118 12:10:13.292010   20212 cache_images.go:84] Images are preloaded, skipping loading
I0118 12:10:13.292010   20212 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0118 12:10:13.292010   20212 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0118 12:10:13.299269   20212 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0118 12:10:13.444199   20212 cni.go:84] Creating CNI manager for ""
I0118 12:10:13.444199   20212 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0118 12:10:13.444199   20212 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0118 12:10:13.444199   20212 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0118 12:10:13.444199   20212 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0118 12:10:13.448314   20212 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0118 12:10:13.457333   20212 binaries.go:44] Found k8s binaries, skipping transfer
I0118 12:10:13.461493   20212 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0118 12:10:13.469343   20212 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0118 12:10:13.483317   20212 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0118 12:10:13.497463   20212 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0118 12:10:13.552539   20212 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0118 12:10:13.556973   20212 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0118 12:10:13.570885   20212 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0118 12:10:13.629070   20212 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0118 12:10:13.689244   20212 certs.go:68] Setting up C:\Users\Programming\.minikube\profiles\minikube for IP: 192.168.49.2
I0118 12:10:13.689244   20212 certs.go:194] generating shared ca certs ...
I0118 12:10:13.689750   20212 certs.go:226] acquiring lock for ca certs: {Name:mkb568819d5c81988a283bd08d67135696e9e1a0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0118 12:10:13.689750   20212 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\Programming\.minikube\ca.key
I0118 12:10:13.689750   20212 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\Programming\.minikube\proxy-client-ca.key
I0118 12:10:13.690279   20212 certs.go:256] generating profile certs ...
I0118 12:10:13.690279   20212 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\Programming\.minikube\profiles\minikube\client.key
I0118 12:10:13.690795   20212 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\Programming\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0118 12:10:13.690795   20212 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\Programming\.minikube\profiles\minikube\proxy-client.key
I0118 12:10:13.691315   20212 certs.go:484] found cert: C:\Users\Programming\.minikube\certs\ca-key.pem (1675 bytes)
I0118 12:10:13.691315   20212 certs.go:484] found cert: C:\Users\Programming\.minikube\certs\ca.pem (1090 bytes)
I0118 12:10:13.691315   20212 certs.go:484] found cert: C:\Users\Programming\.minikube\certs\cert.pem (1135 bytes)
I0118 12:10:13.691315   20212 certs.go:484] found cert: C:\Users\Programming\.minikube\certs\key.pem (1679 bytes)
I0118 12:10:13.692353   20212 ssh_runner.go:362] scp C:\Users\Programming\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0118 12:10:13.714621   20212 ssh_runner.go:362] scp C:\Users\Programming\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0118 12:10:13.736529   20212 ssh_runner.go:362] scp C:\Users\Programming\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0118 12:10:13.779172   20212 ssh_runner.go:362] scp C:\Users\Programming\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0118 12:10:13.801965   20212 ssh_runner.go:362] scp C:\Users\Programming\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0118 12:10:13.825830   20212 ssh_runner.go:362] scp C:\Users\Programming\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0118 12:10:13.888580   20212 ssh_runner.go:362] scp C:\Users\Programming\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0118 12:10:13.910632   20212 ssh_runner.go:362] scp C:\Users\Programming\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0118 12:10:13.931430   20212 ssh_runner.go:362] scp C:\Users\Programming\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0118 12:10:13.951062   20212 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0118 12:10:14.007314   20212 ssh_runner.go:195] Run: openssl version
I0118 12:10:14.021666   20212 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0118 12:10:14.080593   20212 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0118 12:10:14.086936   20212 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jan 18 09:52 /usr/share/ca-certificates/minikubeCA.pem
I0118 12:10:14.140387   20212 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0118 12:10:14.152063   20212 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0118 12:10:14.205563   20212 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0118 12:10:14.249787   20212 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0118 12:10:14.299797   20212 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0118 12:10:14.358494   20212 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0118 12:10:14.410666   20212 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0118 12:10:14.513790   20212 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0118 12:10:14.565521   20212 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0118 12:10:14.573630   20212 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/programming:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0118 12:10:14.579908   20212 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0118 12:10:14.743386   20212 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0118 12:10:14.754049   20212 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0118 12:10:14.754049   20212 kubeadm.go:593] restartPrimaryControlPlane start ...
I0118 12:10:14.757797   20212 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0118 12:10:14.842907   20212 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0118 12:10:14.851912   20212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0118 12:10:14.898930   20212 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in C:\Users\Programming\.kube\configLocal
I0118 12:10:14.898930   20212 kubeconfig.go:62] C:\Users\Programming\.kube\configLocal needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I0118 12:10:14.899455   20212 lock.go:35] WriteFile acquiring C:\Users\Programming\.kube\configLocal: {Name:mkcf8d76818008599506790fd246b5c449ba400a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0118 12:10:14.918261   20212 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0118 12:10:14.937711   20212 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I0118 12:10:14.937711   20212 kubeadm.go:597] duration metric: took 183.6614ms to restartPrimaryControlPlane
I0118 12:10:14.937711   20212 kubeadm.go:394] duration metric: took 364.0808ms to StartCluster
I0118 12:10:14.937711   20212 settings.go:142] acquiring lock: {Name:mkc49103c6448623a336cf220911889a942f12e1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0118 12:10:14.938222   20212 settings.go:150] Updating kubeconfig:  C:\Users\Programming\.kube\configLocal
I0118 12:10:14.939290   20212 lock.go:35] WriteFile acquiring C:\Users\Programming\.kube\configLocal: {Name:mkcf8d76818008599506790fd246b5c449ba400a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0118 12:10:14.940346   20212 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0118 12:10:14.940346   20212 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0118 12:10:14.940346   20212 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0118 12:10:14.940346   20212 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0118 12:10:14.940346   20212 addons.go:247] addon storage-provisioner should already be in state true
I0118 12:10:14.940346   20212 addons.go:69] Setting dashboard=true in profile "minikube"
I0118 12:10:14.940346   20212 addons.go:238] Setting addon dashboard=true in "minikube"
W0118 12:10:14.940346   20212 addons.go:247] addon dashboard should already be in state true
I0118 12:10:14.940346   20212 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0118 12:10:14.940346   20212 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0118 12:10:14.940346   20212 host.go:66] Checking if "minikube" exists ...
I0118 12:10:14.940346   20212 host.go:66] Checking if "minikube" exists ...
I0118 12:10:14.940346   20212 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0118 12:10:14.940346   20212 out.go:177] * Verifying Kubernetes components...
I0118 12:10:14.949829   20212 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0118 12:10:14.961852   20212 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0118 12:10:14.962953   20212 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0118 12:10:14.962953   20212 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0118 12:10:15.014948   20212 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0118 12:10:15.014948   20212 addons.go:247] addon default-storageclass should already be in state true
I0118 12:10:15.014948   20212 host.go:66] Checking if "minikube" exists ...
I0118 12:10:15.026694   20212 out.go:177]   - Using image docker.io/kubernetesui/dashboard:v2.7.0
I0118 12:10:15.028863   20212 out.go:177]   - Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0118 12:10:15.029399   20212 addons.go:435] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0118 12:10:15.029399   20212 ssh_runner.go:362] scp dashboard/dashboard-ns.yaml --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0118 12:10:15.034680   20212 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0118 12:10:15.037306   20212 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0118 12:10:15.038354   20212 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0118 12:10:15.038354   20212 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0118 12:10:15.040519   20212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0118 12:10:15.048421   20212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0118 12:10:15.082342   20212 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0118 12:10:15.082342   20212 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0118 12:10:15.088164   20212 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50731 SSHKeyPath:C:\Users\Programming\.minikube\machines\minikube\id_rsa Username:docker}
I0118 12:10:15.089733   20212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0118 12:10:15.096552   20212 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50731 SSHKeyPath:C:\Users\Programming\.minikube\machines\minikube\id_rsa Username:docker}
I0118 12:10:15.140991   20212 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50731 SSHKeyPath:C:\Users\Programming\.minikube\machines\minikube\id_rsa Username:docker}
I0118 12:10:15.243567   20212 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0118 12:10:15.329072   20212 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0118 12:10:15.333779   20212 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0118 12:10:15.372427   20212 api_server.go:52] waiting for apiserver process to appear ...
I0118 12:10:15.376026   20212 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0118 12:10:15.428878   20212 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0118 12:10:15.428878   20212 ssh_runner.go:362] scp dashboard/dashboard-clusterrole.yaml --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0118 12:10:15.605116   20212 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0118 12:10:15.605116   20212 ssh_runner.go:362] scp dashboard/dashboard-clusterrolebinding.yaml --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0118 12:10:15.609885   20212 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0118 12:10:15.620440   20212 addons.go:435] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0118 12:10:15.620440   20212 ssh_runner.go:362] scp dashboard/dashboard-configmap.yaml --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0118 12:10:15.699137   20212 addons.go:435] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0118 12:10:15.699137   20212 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0118 12:10:15.723267   20212 addons.go:435] installing /etc/kubernetes/addons/dashboard-role.yaml
I0118 12:10:15.723267   20212 ssh_runner.go:362] scp dashboard/dashboard-role.yaml --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0118 12:10:15.815615   20212 addons.go:435] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0118 12:10:15.815615   20212 ssh_runner.go:362] scp dashboard/dashboard-rolebinding.yaml --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0118 12:10:15.896562   20212 addons.go:435] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0118 12:10:15.896562   20212 ssh_runner.go:362] scp dashboard/dashboard-sa.yaml --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0118 12:10:15.913668   20212 addons.go:435] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0118 12:10:15.913668   20212 ssh_runner.go:362] scp dashboard/dashboard-secret.yaml --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0118 12:10:16.005190   20212 addons.go:435] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0118 12:10:16.005190   20212 ssh_runner.go:362] scp dashboard/dashboard-svc.yaml --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0118 12:10:16.089831   20212 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0118 12:10:17.623316   20212 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.2895374s)
I0118 12:10:17.623316   20212 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (2.0134319s)
I0118 12:10:17.623316   20212 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (2.2472907s)
I0118 12:10:17.623316   20212 api_server.go:72] duration metric: took 2.6829706s to wait for apiserver process to appear ...
I0118 12:10:17.623316   20212 api_server.go:88] waiting for apiserver healthz status ...
I0118 12:10:17.623316   20212 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50735/healthz ...
I0118 12:10:17.713622   20212 api_server.go:279] https://127.0.0.1:50735/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0118 12:10:17.713622   20212 api_server.go:103] status: https://127.0.0.1:50735/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0118 12:10:18.123573   20212 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50735/healthz ...
I0118 12:10:18.188398   20212 api_server.go:279] https://127.0.0.1:50735/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0118 12:10:18.188398   20212 api_server.go:103] status: https://127.0.0.1:50735/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0118 12:10:18.624124   20212 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50735/healthz ...
I0118 12:10:18.672222   20212 api_server.go:279] https://127.0.0.1:50735/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0118 12:10:18.672222   20212 api_server.go:103] status: https://127.0.0.1:50735/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0118 12:10:19.123598   20212 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50735/healthz ...
I0118 12:10:19.154373   20212 api_server.go:279] https://127.0.0.1:50735/healthz returned 200:
ok
I0118 12:10:19.245581   20212 api_server.go:141] control plane version: v1.32.0
I0118 12:10:19.245581   20212 api_server.go:131] duration metric: took 1.6222644s to wait for apiserver health ...
I0118 12:10:19.245581   20212 system_pods.go:43] waiting for kube-system pods to appear ...
I0118 12:10:19.257584   20212 system_pods.go:59] 8 kube-system pods found
I0118 12:10:19.257584   20212 system_pods.go:61] "coredns-668d6bf9bc-k9xqz" [6e52f77d-cc63-46ed-9904-c301f3beb062] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0118 12:10:19.257584   20212 system_pods.go:61] "coredns-668d6bf9bc-mdp7b" [fb008fd4-dd9c-4763-bebd-61bf77275094] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0118 12:10:19.257584   20212 system_pods.go:61] "etcd-minikube" [413bb6e6-84b7-47f0-930f-f85c26636d5a] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0118 12:10:19.257584   20212 system_pods.go:61] "kube-apiserver-minikube" [f7e2ec4c-cff7-4e63-a99a-4ebf6ed69fb8] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0118 12:10:19.257584   20212 system_pods.go:61] "kube-controller-manager-minikube" [23363e13-8c5b-4db3-ab58-dcd7dd0bd555] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0118 12:10:19.257584   20212 system_pods.go:61] "kube-proxy-mh85t" [2e8ee376-ce83-488f-83dc-132a1d5e1856] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0118 12:10:19.257584   20212 system_pods.go:61] "kube-scheduler-minikube" [19ffd113-840b-4f7c-8fe2-5cef44509475] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0118 12:10:19.257584   20212 system_pods.go:61] "storage-provisioner" [58d64d45-065e-4d44-9cc8-9bcbe78c1824] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0118 12:10:19.257584   20212 system_pods.go:74] duration metric: took 12.0034ms to wait for pod list to return data ...
I0118 12:10:19.257584   20212 kubeadm.go:582] duration metric: took 4.3172384s to wait for: map[apiserver:true system_pods:true]
I0118 12:10:19.257584   20212 node_conditions.go:102] verifying NodePressure condition ...
I0118 12:10:19.349729   20212 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0118 12:10:19.349729   20212 node_conditions.go:123] node cpu capacity is 16
I0118 12:10:19.350343   20212 node_conditions.go:105] duration metric: took 92.7583ms to run NodePressure ...
I0118 12:10:19.350343   20212 start.go:241] waiting for startup goroutines ...
I0118 12:10:20.502254   20212 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (4.4124227s)
I0118 12:10:20.503303   20212 out.go:177] * Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server

I0118 12:10:20.504862   20212 out.go:177] * Enabled addons: storage-provisioner, default-storageclass, dashboard
I0118 12:10:20.506447   20212 addons.go:514] duration metric: took 5.5661014s for enable addons: enabled=[storage-provisioner default-storageclass dashboard]
I0118 12:10:20.506447   20212 start.go:246] waiting for cluster config update ...
I0118 12:10:20.506447   20212 start.go:255] writing updated cluster config ...
I0118 12:10:20.548905   20212 ssh_runner.go:195] Run: rm -f paused
I0118 12:10:20.713526   20212 start.go:600] kubectl: 1.32.3, cluster: 1.32.0 (minor skew: 0)
I0118 12:10:20.714033   20212 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jan 18 14:11:07 minikube dockerd[990]: time="2026-01-18T14:11:07.390586065Z" level=info msg="ignoring event" container=c6372845a6012571968081332045fb6725906a1e47a2d13df10161bcbb6f1953 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 18 14:16:50 minikube cri-dockerd[1293]: time="2026-01-18T14:16:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/46ceffa20ebea27be7a04f0be3e9d83ff8ea46c8c10509a933fdb1b77d7e6989/resolv.conf as [nameserver 10.96.0.10 search 12000000-0000-0000-0000-000000000000.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 18 14:16:52 minikube dockerd[990]: time="2026-01-18T14:16:52.165767839Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Jan 18 14:17:16 minikube dockerd[990]: time="2026-01-18T14:17:16.830001760Z" level=info msg="Container failed to exit within 30s of signal 3 - using the force" container=33469457fd0b7d9a32bb7a64bc763ab0181ac99eb212219d968214dae858b6f8
Jan 18 14:17:16 minikube dockerd[990]: time="2026-01-18T14:17:16.942638982Z" level=info msg="ignoring event" container=33469457fd0b7d9a32bb7a64bc763ab0181ac99eb212219d968214dae858b6f8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 18 14:17:17 minikube dockerd[990]: time="2026-01-18T14:17:17.223458548Z" level=info msg="ignoring event" container=e6b33791c850ef5b6ec18ef2d5ab531bd8acc390e33e5ac52f442f443c66f689 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 18 14:17:17 minikube dockerd[990]: time="2026-01-18T14:17:17.979599740Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Jan 18 14:17:19 minikube cri-dockerd[1293]: time="2026-01-18T14:17:19Z" level=error msg="error getting RW layer size for container ID '33469457fd0b7d9a32bb7a64bc763ab0181ac99eb212219d968214dae858b6f8': Error response from daemon: No such container: 33469457fd0b7d9a32bb7a64bc763ab0181ac99eb212219d968214dae858b6f8"
Jan 18 14:17:19 minikube cri-dockerd[1293]: time="2026-01-18T14:17:19Z" level=error msg="Set backoffDuration to : 1m0s for container ID '33469457fd0b7d9a32bb7a64bc763ab0181ac99eb212219d968214dae858b6f8'"
Jan 18 14:17:44 minikube dockerd[990]: time="2026-01-18T14:17:44.648584488Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Jan 18 14:18:39 minikube dockerd[990]: time="2026-01-18T14:18:39.116101308Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Jan 18 14:20:07 minikube dockerd[990]: time="2026-01-18T14:20:07.275151824Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Jan 18 14:22:48 minikube dockerd[990]: time="2026-01-18T14:22:48.567290997Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Jan 18 14:23:45 minikube dockerd[990]: time="2026-01-18T14:23:45.029548922Z" level=info msg="ignoring event" container=46ceffa20ebea27be7a04f0be3e9d83ff8ea46c8c10509a933fdb1b77d7e6989 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 18 14:24:56 minikube cri-dockerd[1293]: time="2026-01-18T14:24:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9581dea83c6ed191c0cdb8f685ddf92af910f882b9f008a576d963ecb7384929/resolv.conf as [nameserver 10.96.0.10 search 12000000-0000-0000-0000-000000000000.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 18 14:25:09 minikube cri-dockerd[1293]: time="2026-01-18T14:25:09Z" level=info msg="Pulling image docker.io/rabbitmq:4-management: d7cb76db81cb: Downloading [=====================================>             ]  6.762MB/8.995MB"
Jan 18 14:25:19 minikube cri-dockerd[1293]: time="2026-01-18T14:25:19Z" level=info msg="Pulling image docker.io/rabbitmq:4-management: b605fd694c0f: Downloading [===============================>                   ]  28.95MB/46.26MB"
Jan 18 14:25:28 minikube cri-dockerd[1293]: time="2026-01-18T14:25:28Z" level=info msg="Pulling image docker.io/rabbitmq:4-management: ded6a69a96ac: Download complete "
Jan 18 14:25:38 minikube cri-dockerd[1293]: time="2026-01-18T14:25:38Z" level=info msg="Pulling image docker.io/rabbitmq:4-management: 35332e1fcae4: Download complete "
Jan 18 14:25:47 minikube cri-dockerd[1293]: time="2026-01-18T14:25:47Z" level=info msg="Pulling image docker.io/rabbitmq:4-management: 35332e1fcae4: Download complete "
Jan 18 14:25:56 minikube cri-dockerd[1293]: time="2026-01-18T14:25:56Z" level=info msg="Pulling image docker.io/rabbitmq:4-management: 35332e1fcae4: Download complete "
Jan 18 14:25:57 minikube dockerd[990]: time="2026-01-18T14:25:57.228382981Z" level=info msg="Download failed, retrying (1/5): dial tcp 172.64.66.1:443: connect: connection refused"
Jan 18 14:26:06 minikube cri-dockerd[1293]: time="2026-01-18T14:26:06Z" level=info msg="Pulling image docker.io/rabbitmq:4-management: 6f813129d9f1: Downloading [=======================>                           ]  12.95MB/27.88MB"
Jan 18 14:26:09 minikube dockerd[990]: time="2026-01-18T14:26:09.934544315Z" level=info msg="Download failed, retrying (1/5): dial tcp 172.64.66.1:443: connect: connection refused"
Jan 18 14:26:16 minikube cri-dockerd[1293]: time="2026-01-18T14:26:16Z" level=info msg="Pulling image docker.io/rabbitmq:4-management: b2e2cb4101ff: Retrying in 1 second "
Jan 18 14:26:25 minikube cri-dockerd[1293]: time="2026-01-18T14:26:25Z" level=info msg="Pulling image docker.io/rabbitmq:4-management: b2e2cb4101ff: Retrying in 1 second "
Jan 18 14:26:35 minikube cri-dockerd[1293]: time="2026-01-18T14:26:35Z" level=info msg="Pulling image docker.io/rabbitmq:4-management: b2e2cb4101ff: Retrying in 1 second "
Jan 18 14:26:40 minikube cri-dockerd[1293]: time="2026-01-18T14:26:40Z" level=info msg="Stop pulling image docker.io/rabbitmq:4-management: Status: Downloaded newer image for rabbitmq:4-management"
Jan 18 14:26:40 minikube dockerd[990]: time="2026-01-18T14:26:40.957373064Z" level=info msg="ignoring event" container=85ed589b792cd6c6ab16b7a81e85abf1bc6081f1dc127d43d74c573a3469385d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 18 14:26:41 minikube dockerd[990]: time="2026-01-18T14:26:41.448040837Z" level=info msg="ignoring event" container=86e055cb86d9aa14b20296f576f0e9ae07a3c71ab2ccbe1542ce47f31ef32bf8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 18 14:26:55 minikube dockerd[990]: time="2026-01-18T14:26:55.265098192Z" level=info msg="ignoring event" container=70c2f821ea41818bd134baf37ebb49b2efc0ed74222986956c6a5e211a194f7a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 18 14:27:20 minikube dockerd[990]: time="2026-01-18T14:27:20.179591079Z" level=info msg="ignoring event" container=68a2864b94732e760cfe72432790f2ddc5086f61e3128d0e629d391ea18f8276 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 18 14:28:03 minikube dockerd[990]: time="2026-01-18T14:28:03.628909164Z" level=info msg="ignoring event" container=ed4a4e7b44633ffc350643f2d996bf10b90cbcdd4892fa6b053cc2cbf45899e8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 18 14:29:27 minikube dockerd[990]: time="2026-01-18T14:29:27.842907826Z" level=info msg="ignoring event" container=db82af34832a427a1b2fd8754fec0d6b06883771db1d56dd69d1b85084771917 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 18 14:32:09 minikube dockerd[990]: time="2026-01-18T14:32:09.352440478Z" level=info msg="ignoring event" container=b3c0bb65f8b754135a5170314a8ebfdbba831339fc1579eb258ee7156396951c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 18 14:34:17 minikube dockerd[990]: time="2026-01-18T14:34:17.128487718Z" level=info msg="ignoring event" container=9581dea83c6ed191c0cdb8f685ddf92af910f882b9f008a576d963ecb7384929 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 18 14:39:10 minikube cri-dockerd[1293]: time="2026-01-18T14:39:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/17c6ebfab9e7c1f27e778410850752057da0a6da7b67ebca3bb235026818e394/resolv.conf as [nameserver 10.96.0.10 search 12000000-0000-0000-0000-000000000000.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 18 14:39:12 minikube dockerd[990]: time="2026-01-18T14:39:12.447205794Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 18 14:39:12 minikube dockerd[990]: time="2026-01-18T14:39:12.447315413Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 18 14:39:28 minikube dockerd[990]: time="2026-01-18T14:39:28.697926664Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 18 14:39:28 minikube dockerd[990]: time="2026-01-18T14:39:28.698010400Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 18 14:39:56 minikube dockerd[990]: time="2026-01-18T14:39:56.161669065Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 18 14:39:56 minikube dockerd[990]: time="2026-01-18T14:39:56.161715554Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 18 14:40:41 minikube dockerd[990]: time="2026-01-18T14:40:41.008311942Z" level=info msg="ignoring event" container=17c6ebfab9e7c1f27e778410850752057da0a6da7b67ebca3bb235026818e394 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 18 14:40:41 minikube cri-dockerd[1293]: time="2026-01-18T14:40:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/92281680d52dbedcae9d6e1e5d5642ce46f3e47dd290c0c900a5fc8cac1eccdf/resolv.conf as [nameserver 10.96.0.10 search 12000000-0000-0000-0000-000000000000.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 18 14:40:43 minikube dockerd[990]: time="2026-01-18T14:40:43.352505378Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 18 14:40:43 minikube dockerd[990]: time="2026-01-18T14:40:43.352555202Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 18 14:40:59 minikube dockerd[990]: time="2026-01-18T14:40:59.555807513Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 18 14:40:59 minikube dockerd[990]: time="2026-01-18T14:40:59.555863733Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 18 14:41:29 minikube dockerd[990]: time="2026-01-18T14:41:29.191617180Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 18 14:41:29 minikube dockerd[990]: time="2026-01-18T14:41:29.191666754Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 18 14:42:11 minikube dockerd[990]: time="2026-01-18T14:42:11.111214566Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 18 14:42:11 minikube dockerd[990]: time="2026-01-18T14:42:11.111262020Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 18 14:43:30 minikube dockerd[990]: time="2026-01-18T14:43:30.672306213Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 18 14:43:30 minikube dockerd[990]: time="2026-01-18T14:43:30.672360374Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 18 14:46:06 minikube dockerd[990]: time="2026-01-18T14:46:06.007000686Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 18 14:46:06 minikube dockerd[990]: time="2026-01-18T14:46:06.007049438Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 18 14:46:20 minikube cri-dockerd[1293]: time="2026-01-18T14:46:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/851d6f353fdeb203787d218ee176697af5e6a98131ec5d03274f0b2505671d32/resolv.conf as [nameserver 10.96.0.10 search 12000000-0000-0000-0000-000000000000.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 18 14:51:05 minikube dockerd[990]: time="2026-01-18T14:51:05.733936417Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 18 14:51:05 minikube dockerd[990]: time="2026-01-18T14:51:05.734008539Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE                                                                                                         CREATED             STATE               NAME                               ATTEMPT             POD ID              POD
99af497dd48ae       public.ecr.aws/docker/library/redis@sha256:59b6e694653476de2c992937ebe1c64182af4728e54bb49e9b7a6c26614d8933   5 hours ago         Running             redis                              0                   5d955fab7712b       argocd-redis-c98d5794d-qjwft
548351a6e8ffb       quay.io/argoproj/argocd@sha256:090c287bdc16b4649d3fe30e2fa73633debe115cf40d182d4a0a9f125695b64a               5 hours ago         Running             argocd-repo-server                 0                   ba1385fb0fc7e       argocd-repo-server-7f86545bc4-jzk4z
d01a04f7d25a6       ghcr.io/dexidp/dex@sha256:b08a58c9731c693b8db02154d7afda798e1888dc76db30d34c4a0d0b8a26d913                    5 hours ago         Running             dex                                0                   fbc65e853014e       argocd-dex-server-6944b95798-w77sb
71235619d8884       quay.io/argoproj/argocd@sha256:090c287bdc16b4649d3fe30e2fa73633debe115cf40d182d4a0a9f125695b64a               5 hours ago         Running             argocd-application-controller      0                   35bf65c9b43c4       argocd-application-controller-0
3cae6fc30c732       quay.io/argoproj/argocd@sha256:090c287bdc16b4649d3fe30e2fa73633debe115cf40d182d4a0a9f125695b64a               5 hours ago         Running             argocd-server                      0                   bb8ee1186b27c       argocd-server-685f5fb66f-txrx9
f526ea638350c       quay.io/argoproj/argocd@sha256:090c287bdc16b4649d3fe30e2fa73633debe115cf40d182d4a0a9f125695b64a               5 hours ago         Running             argocd-notifications-controller    0                   5b2f96581735b       argocd-notifications-controller-7f5b87f55b-q5rvg
add507ccc65b1       quay.io/argoproj/argocd@sha256:090c287bdc16b4649d3fe30e2fa73633debe115cf40d182d4a0a9f125695b64a               5 hours ago         Exited              copyutil                           0                   ba1385fb0fc7e       argocd-repo-server-7f86545bc4-jzk4z
e80a0a40cd7ea       quay.io/argoproj/argocd@sha256:090c287bdc16b4649d3fe30e2fa73633debe115cf40d182d4a0a9f125695b64a               5 hours ago         Running             argocd-applicationset-controller   0                   7afc4fc623430       argocd-applicationset-controller-944684d77-nnbzm
a81701d160597       quay.io/argoproj/argocd@sha256:090c287bdc16b4649d3fe30e2fa73633debe115cf40d182d4a0a9f125695b64a               5 hours ago         Exited              secret-init                        0                   5d955fab7712b       argocd-redis-c98d5794d-qjwft
98a302aa1a20e       quay.io/argoproj/argocd@sha256:090c287bdc16b4649d3fe30e2fa73633debe115cf40d182d4a0a9f125695b64a               5 hours ago         Exited              copyutil                           0                   fbc65e853014e       argocd-dex-server-6944b95798-w77sb
8699e0eb553ec       07655ddf2eebe                                                                                                 5 hours ago         Running             kubernetes-dashboard               2                   9eaf065689062       kubernetes-dashboard-7779f9b69b-q4ct9
06e20ae45c90a       6e38f40d628db                                                                                                 5 hours ago         Running             storage-provisioner                3                   3fbf6f79fce2e       storage-provisioner
41b51ede4d64b       c69fa2e9cbf5f                                                                                                 5 hours ago         Running             coredns                            1                   3d41a77e04c67       coredns-668d6bf9bc-k9xqz
60242ad2d512c       c69fa2e9cbf5f                                                                                                 5 hours ago         Running             coredns                            1                   5deeff0fca0ea       coredns-668d6bf9bc-mdp7b
783c099679107       115053965e86b                                                                                                 5 hours ago         Running             dashboard-metrics-scraper          1                   ceb5a2583236a       dashboard-metrics-scraper-5d59dccf9b-6cqx5
ba7f20314ca74       07655ddf2eebe                                                                                                 5 hours ago         Exited              kubernetes-dashboard               1                   9eaf065689062       kubernetes-dashboard-7779f9b69b-q4ct9
f8c6a5e5c3d0d       040f9f8aac8cd                                                                                                 5 hours ago         Running             kube-proxy                         1                   88d59e99102b0       kube-proxy-mh85t
f7e383d6625e9       6e38f40d628db                                                                                                 5 hours ago         Exited              storage-provisioner                2                   3fbf6f79fce2e       storage-provisioner
a2be7b52b2926       a389e107f4ff1                                                                                                 5 hours ago         Running             kube-scheduler                     1                   ca4b40550ab02       kube-scheduler-minikube
e58ccc01aea9f       a9e7e6b294baf                                                                                                 5 hours ago         Running             etcd                               1                   81a1a934356cb       etcd-minikube
405586ba8e52d       8cab3d2a8bd0f                                                                                                 5 hours ago         Running             kube-controller-manager            1                   4c9b62645e3e2       kube-controller-manager-minikube
fe61d44587426       c2e17b8d0f4a3                                                                                                 5 hours ago         Running             kube-apiserver                     1                   cc9a0b4b98ac8       kube-apiserver-minikube
a5aec8b257d70       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c          5 hours ago         Exited              dashboard-metrics-scraper          0                   156cf9b909455       dashboard-metrics-scraper-5d59dccf9b-6cqx5
2f23c32f3bc64       c69fa2e9cbf5f                                                                                                 5 hours ago         Exited              coredns                            0                   17345be8966b6       coredns-668d6bf9bc-mdp7b
b0dc04c92db59       c69fa2e9cbf5f                                                                                                 5 hours ago         Exited              coredns                            0                   d0eb826f85277       coredns-668d6bf9bc-k9xqz
fff7680f55663       040f9f8aac8cd                                                                                                 5 hours ago         Exited              kube-proxy                         0                   635c212362c34       kube-proxy-mh85t
54a611dd5f881       a9e7e6b294baf                                                                                                 5 hours ago         Exited              etcd                               0                   a924740dcdb77       etcd-minikube
aa464cc92e4b9       8cab3d2a8bd0f                                                                                                 5 hours ago         Exited              kube-controller-manager            0                   68bed4ea37d12       kube-controller-manager-minikube
a11727c2cc11c       c2e17b8d0f4a3                                                                                                 5 hours ago         Exited              kube-apiserver                     0                   0513be07054ee       kube-apiserver-minikube
98c23fd63bbc7       a389e107f4ff1                                                                                                 5 hours ago         Exited              kube-scheduler                     0                   4a197b0ea4170       kube-scheduler-minikube


==> coredns [2f23c32f3bc6] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [41b51ede4d64] <==
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1250430703]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (18-Jan-2026 10:10:20.314) (total time: 22222ms):
Trace[1250430703]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 22218ms (10:10:42.533)
Trace[1250430703]: [22.222984628s] [22.222984628s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[951501573]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (18-Jan-2026 10:10:20.314) (total time: 22234ms):
Trace[951501573]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 22234ms (10:10:42.549)
Trace[951501573]: [22.234800967s] [22.234800967s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[948182327]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (18-Jan-2026 10:10:20.315) (total time: 22234ms):
Trace[948182327]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 22234ms (10:10:42.549)
Trace[948182327]: [22.234341066s] [22.234341066s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/errors: 2 _grpc_config.localhost. TXT: read udp 10.244.0.7:48670->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 redis-master. A: read udp 10.244.0.7:51689->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 redis-master. A: read udp 10.244.0.7:40281->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 redis-master. A: read udp 10.244.0.7:46929->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 redis-master. A: read udp 10.244.0.7:56951->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 redis-master. A: read udp 10.244.0.7:57267->192.168.65.254:53: i/o timeout


==> coredns [60242ad2d512] <==
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1405888543]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (18-Jan-2026 10:10:20.317) (total time: 22220ms):
Trace[1405888543]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 22215ms (10:10:42.533)
Trace[1405888543]: [22.22040388s] [22.22040388s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[328912436]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (18-Jan-2026 10:10:20.317) (total time: 22220ms):
Trace[328912436]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 22215ms (10:10:42.533)
Trace[328912436]: [22.220515883s] [22.220515883s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[643650042]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (18-Jan-2026 10:10:20.317) (total time: 22220ms):
Trace[643650042]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 22215ms (10:10:42.533)
Trace[643650042]: [22.220681218s] [22.220681218s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/ready: Still waiting on: "kubernetes"
[ERROR] plugin/errors: 2 redis-master. A: read udp 10.244.0.9:52176->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 redis-master. A: read udp 10.244.0.9:37235->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 redis-master. A: read udp 10.244.0.9:33182->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 redis-master. A: read udp 10.244.0.9:59851->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 redis-master. A: read udp 10.244.0.9:34248->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 redis-master. A: read udp 10.244.0.9:55111->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 redis-master. A: read udp 10.244.0.9:41089->192.168.65.254:53: i/o timeout


==> coredns [b0dc04c92db5] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2026_01_18T11_52_46_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 18 Jan 2026 09:52:43 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 18 Jan 2026 14:53:42 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 18 Jan 2026 14:51:44 +0000   Sun, 18 Jan 2026 09:52:42 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 18 Jan 2026 14:51:44 +0000   Sun, 18 Jan 2026 09:52:42 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 18 Jan 2026 14:51:44 +0000   Sun, 18 Jan 2026 09:52:42 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 18 Jan 2026 14:51:44 +0000   Sun, 18 Jan 2026 09:52:43 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16292028Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16292028Ki
  pods:               110
System Info:
  Machine ID:                           3242c2eacb384abe9e2dc17276616135
  System UUID:                          3242c2eacb384abe9e2dc17276616135
  Boot ID:                              40241298-6d97-46e0-8d3c-f3e43d55a3a4
  Kernel Version:                       6.6.87.2-microsoft-standard-WSL2
  OS Image:                             Ubuntu 22.04.5 LTS
  Operating System:                     linux
  Architecture:                         amd64
  Container Runtime Version:            docker://27.4.1
  Kubelet Version:                      v1.32.0
  Kube-Proxy Version:                   v1.32.0
PodCIDR:                                10.244.0.0/24
PodCIDRs:                               10.244.0.0/24
Non-terminated Pods:                    (19 in total)
  Namespace                             Name                                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                             ----                                                  ------------  ----------  ---------------  -------------  ---
  12000000-0000-0000-0000-000000000000  ibraheem103-guestbook-weather-api-66c9b9bfc8-v4hn4    0 (0%)        0 (0%)      0 (0%)           0 (0%)         13m
  12000000-0000-0000-0000-000000000000  ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h     0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m28s
  argocd                                argocd-application-controller-0                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h41m
  argocd                                argocd-applicationset-controller-944684d77-nnbzm      0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h41m
  argocd                                argocd-dex-server-6944b95798-w77sb                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h41m
  argocd                                argocd-notifications-controller-7f5b87f55b-q5rvg      0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h41m
  argocd                                argocd-redis-c98d5794d-qjwft                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h41m
  argocd                                argocd-repo-server-7f86545bc4-jzk4z                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h41m
  argocd                                argocd-server-685f5fb66f-txrx9                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h41m
  kube-system                           coredns-668d6bf9bc-k9xqz                              100m (0%)     0 (0%)      70Mi (0%)        170Mi (1%)     5h
  kube-system                           coredns-668d6bf9bc-mdp7b                              100m (0%)     0 (0%)      70Mi (0%)        170Mi (1%)     5h
  kube-system                           etcd-minikube                                         100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         5h1m
  kube-system                           kube-apiserver-minikube                               250m (1%)     0 (0%)      0 (0%)           0 (0%)         5h1m
  kube-system                           kube-controller-manager-minikube                      200m (1%)     0 (0%)      0 (0%)           0 (0%)         5h1m
  kube-system                           kube-proxy-mh85t                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h
  kube-system                           kube-scheduler-minikube                               100m (0%)     0 (0%)      0 (0%)           0 (0%)         5h1m
  kube-system                           storage-provisioner                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h1m
  kubernetes-dashboard                  dashboard-metrics-scraper-5d59dccf9b-6cqx5            0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h
  kubernetes-dashboard                  kubernetes-dashboard-7779f9b69b-q4ct9                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (5%)   0 (0%)
  memory             240Mi (1%)  340Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:              <none>


==> dmesg <==
[Jan18 09:35] Hyper-V: Disabling IBT because of Hyper-V bug
[  +0.061654] PCI: Fatal: No config space access function found
[  +0.022926] PCI: System does not support PCI
[  +0.097581] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +2.657952] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2058: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.015731] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Africa/Cairo not found. Is the tzdata package installed?
[  +0.145231] pulseaudio[334]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +0.120335] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.006055] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000954] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000658] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000880] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.475952] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.135605] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000706] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000623] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001642] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000780] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000672] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000598] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000507] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000818] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001396] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000602] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.082051] netlink: 'initd': attribute type 4 has an invalid length.
[  +0.172771] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.028003] virtiofs: Unknown parameter 'negative_dentry_timeout'
[  +2.625517] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.007796] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000658] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000605] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000698] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.010534] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.078414] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.007082] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002974] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001335] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001081] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001457] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000876] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001194] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001591] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000808] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001423] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.046508] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.215430] Failed to connect to bus: No such file or directory
[  +0.143053] WSL (267) ERROR: CheckConnection: getaddrinfo() failed: -5
[  +0.102145] systemd-journald[56]: File /var/log/journal/247657c843094d46846ad7472fbf3f28/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Jan18 11:10] WSL (267) ERROR: CheckConnection: getaddrinfo() failed: -5
[Jan18 11:12] WSL (267) ERROR: CheckConnection: getaddrinfo() failed: -3
[  +4.952569] WSL (267) ERROR: CheckConnection: getaddrinfo() failed: -3
[Jan18 13:42] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Africa/Cairo not found. Is the tzdata package installed?


==> etcd [54a611dd5f88] <==
{"level":"info","ts":"2026-01-18T09:52:42.011872Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2026-01-18T09:52:42.015170Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2026-01-18T09:52:42.017891Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2026-01-18T09:52:42.018142Z","caller":"etcdserver/server.go:757","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2026-01-18T09:52:42.018232Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2026-01-18T09:52:42.018330Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2026-01-18T09:52:42.018339Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2026-01-18T09:52:42.018703Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2026-01-18T09:52:42.019897Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2026-01-18T09:52:42.020075Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2026-01-18T09:52:42.020640Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2026-01-18T09:52:42.020778Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2026-01-18T09:52:42.020863Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2026-01-18T09:52:42.021281Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2026-01-18T09:52:42.021321Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2026-01-18T09:52:42.600668Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2026-01-18T09:52:42.600750Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2026-01-18T09:52:42.600763Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2026-01-18T09:52:42.600786Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2026-01-18T09:52:42.600792Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2026-01-18T09:52:42.600797Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2026-01-18T09:52:42.600801Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2026-01-18T09:52:42.602501Z","caller":"etcdserver/server.go:2651","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2026-01-18T09:52:42.603819Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2026-01-18T09:52:42.603833Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2026-01-18T09:52:42.603898Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2026-01-18T09:52:42.604130Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2026-01-18T09:52:42.604155Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2026-01-18T09:52:42.604758Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2026-01-18T09:52:42.604755Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2026-01-18T09:52:42.604758Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2026-01-18T09:52:42.605083Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2026-01-18T09:52:42.605135Z","caller":"etcdserver/server.go:2675","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2026-01-18T09:52:42.605539Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2026-01-18T09:52:42.605676Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2026-01-18T09:53:18.501536Z","caller":"traceutil/trace.go:171","msg":"trace[1544582240] transaction","detail":"{read_only:false; response_revision:463; number_of_response:1; }","duration":"101.770901ms","start":"2026-01-18T09:53:18.399750Z","end":"2026-01-18T09:53:18.501521Z","steps":["trace[1544582240] 'process raft request'  (duration: 101.678671ms)"],"step_count":1}
{"level":"info","ts":"2026-01-18T09:58:54.252233Z","caller":"traceutil/trace.go:171","msg":"trace[1872335113] transaction","detail":"{read_only:false; response_revision:750; number_of_response:1; }","duration":"131.734829ms","start":"2026-01-18T09:58:54.120484Z","end":"2026-01-18T09:58:54.252218Z","steps":["trace[1872335113] 'process raft request'  (duration: 131.646805ms)"],"step_count":1}
{"level":"info","ts":"2026-01-18T10:00:04.518367Z","caller":"traceutil/trace.go:171","msg":"trace[1611908977] linearizableReadLoop","detail":"{readStateIndex:911; appliedIndex:910; }","duration":"108.658561ms","start":"2026-01-18T10:00:04.409668Z","end":"2026-01-18T10:00:04.518326Z","steps":["trace[1611908977] 'read index received'  (duration: 108.212004ms)","trace[1611908977] 'applied index is now lower than readState.Index'  (duration: 441.241s)"],"step_count":2}
{"level":"info","ts":"2026-01-18T10:00:04.518572Z","caller":"traceutil/trace.go:171","msg":"trace[170369277] transaction","detail":"{read_only:false; response_revision:807; number_of_response:1; }","duration":"149.481569ms","start":"2026-01-18T10:00:04.369060Z","end":"2026-01-18T10:00:04.518542Z","steps":["trace[170369277] 'process raft request'  (duration: 148.946865ms)"],"step_count":1}
{"level":"warn","ts":"2026-01-18T10:00:04.518615Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"108.898977ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/validatingadmissionpolicybindings/\" range_end:\"/registry/validatingadmissionpolicybindings0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-01-18T10:00:04.518721Z","caller":"traceutil/trace.go:171","msg":"trace[1248187917] range","detail":"{range_begin:/registry/validatingadmissionpolicybindings/; range_end:/registry/validatingadmissionpolicybindings0; response_count:0; response_revision:807; }","duration":"109.068014ms","start":"2026-01-18T10:00:04.409638Z","end":"2026-01-18T10:00:04.518706Z","steps":["trace[1248187917] 'agreement among raft nodes before linearized reading'  (duration: 108.886417ms)"],"step_count":1}
{"level":"info","ts":"2026-01-18T10:02:07.187448Z","caller":"traceutil/trace.go:171","msg":"trace[1823452342] transaction","detail":"{read_only:false; response_revision:908; number_of_response:1; }","duration":"385.102208ms","start":"2026-01-18T10:02:06.802318Z","end":"2026-01-18T10:02:07.187420Z","steps":["trace[1823452342] 'process raft request'  (duration: 384.919376ms)"],"step_count":1}
{"level":"warn","ts":"2026-01-18T10:02:07.188372Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2026-01-18T10:02:06.802297Z","time spent":"385.271192ms","remote":"127.0.0.1:56864","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:906 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2026-01-18T10:02:20.753462Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":682}
{"level":"info","ts":"2026-01-18T10:02:20.759275Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":682,"took":"5.473901ms","hash":3250534179,"current-db-size-bytes":1736704,"current-db-size":"1.7 MB","current-db-size-in-use-bytes":1736704,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2026-01-18T10:02:20.759327Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3250534179,"revision":682,"compact-revision":-1}
{"level":"info","ts":"2026-01-18T10:05:56.812935Z","caller":"traceutil/trace.go:171","msg":"trace[951246990] transaction","detail":"{read_only:false; response_revision:1099; number_of_response:1; }","duration":"288.272677ms","start":"2026-01-18T10:05:56.524518Z","end":"2026-01-18T10:05:56.812791Z","steps":["trace[951246990] 'process raft request'  (duration: 288.012168ms)"],"step_count":1}
{"level":"info","ts":"2026-01-18T10:07:08.090603Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":920}
{"level":"info","ts":"2026-01-18T10:07:08.094862Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":920,"took":"3.99581ms","hash":618634257,"current-db-size-bytes":1736704,"current-db-size":"1.7 MB","current-db-size-in-use-bytes":1093632,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2026-01-18T10:07:08.094917Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":618634257,"revision":920,"compact-revision":682}
{"level":"info","ts":"2026-01-18T10:09:43.364920Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2026-01-18T10:09:43.365021Z","caller":"embed/etcd.go:378","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2026-01-18T10:09:43.366251Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2026-01-18T10:09:43.366396Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2026-01-18T10:09:43.375015Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2026-01-18T10:09:43.375091Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2026-01-18T10:09:43.376721Z","caller":"etcdserver/server.go:1543","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2026-01-18T10:09:43.381083Z","caller":"embed/etcd.go:582","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2026-01-18T10:09:43.381303Z","caller":"embed/etcd.go:587","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2026-01-18T10:09:43.381353Z","caller":"embed/etcd.go:380","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [e58ccc01aea9] <==
{"level":"info","ts":"2026-01-18T13:44:17.811585Z","caller":"traceutil/trace.go:171","msg":"trace[1097872289] range","detail":"{range_begin:/registry/pods/12000000-0000-0000-0000-000000000000/ibraheem101-guestbook-6949b89b55-xv8zv; range_end:; response_count:1; response_revision:12913; }","duration":"129.116529ms","start":"2026-01-18T13:44:17.682455Z","end":"2026-01-18T13:44:17.811571Z","steps":["trace[1097872289] 'agreement among raft nodes before linearized reading'  (duration: 126.203882ms)"],"step_count":1}
{"level":"info","ts":"2026-01-18T13:44:17.812019Z","caller":"traceutil/trace.go:171","msg":"trace[414637801] range","detail":"{range_begin:/registry/ranges/serviceips; range_end:; response_count:1; response_revision:12913; }","duration":"121.783249ms","start":"2026-01-18T13:44:17.690224Z","end":"2026-01-18T13:44:17.812007Z","steps":["trace[414637801] 'agreement among raft nodes before linearized reading'  (duration: 118.423851ms)"],"step_count":1}
{"level":"info","ts":"2026-01-18T13:49:03.417401Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12860}
{"level":"info","ts":"2026-01-18T13:49:03.429098Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":12860,"took":"11.256176ms","hash":303022210,"current-db-size-bytes":8245248,"current-db-size":"8.2 MB","current-db-size-in-use-bytes":4489216,"current-db-size-in-use":"4.5 MB"}
{"level":"info","ts":"2026-01-18T13:49:03.429167Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":303022210,"revision":12860,"compact-revision":12576}
{"level":"info","ts":"2026-01-18T13:53:50.801810Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13260}
{"level":"info","ts":"2026-01-18T13:53:50.807108Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":13260,"took":"4.753519ms","hash":1860144016,"current-db-size-bytes":8245248,"current-db-size":"8.2 MB","current-db-size-in-use-bytes":4915200,"current-db-size-in-use":"4.9 MB"}
{"level":"info","ts":"2026-01-18T13:53:50.807181Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1860144016,"revision":13260,"compact-revision":12860}
{"level":"info","ts":"2026-01-18T13:58:37.538854Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13680}
{"level":"info","ts":"2026-01-18T13:58:37.543373Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":13680,"took":"4.044085ms","hash":1709377489,"current-db-size-bytes":8245248,"current-db-size":"8.2 MB","current-db-size-in-use-bytes":4456448,"current-db-size-in-use":"4.5 MB"}
{"level":"info","ts":"2026-01-18T13:58:37.543444Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1709377489,"revision":13680,"compact-revision":13260}
{"level":"info","ts":"2026-01-18T14:03:24.356395Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14024}
{"level":"info","ts":"2026-01-18T14:03:24.360584Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":14024,"took":"3.855935ms","hash":1337505914,"current-db-size-bytes":8245248,"current-db-size":"8.2 MB","current-db-size-in-use-bytes":4096000,"current-db-size-in-use":"4.1 MB"}
{"level":"info","ts":"2026-01-18T14:03:24.360622Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1337505914,"revision":14024,"compact-revision":13680}
{"level":"info","ts":"2026-01-18T14:08:11.862552Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14265}
{"level":"info","ts":"2026-01-18T14:08:11.866894Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":14265,"took":"3.985082ms","hash":1844055742,"current-db-size-bytes":8245248,"current-db-size":"8.2 MB","current-db-size-in-use-bytes":3366912,"current-db-size-in-use":"3.4 MB"}
{"level":"info","ts":"2026-01-18T14:08:11.866939Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1844055742,"revision":14265,"compact-revision":14024}
{"level":"info","ts":"2026-01-18T14:12:58.814948Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14506}
{"level":"info","ts":"2026-01-18T14:12:58.817933Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":14506,"took":"2.651453ms","hash":3236113679,"current-db-size-bytes":8245248,"current-db-size":"8.2 MB","current-db-size-in-use-bytes":4042752,"current-db-size-in-use":"4.0 MB"}
{"level":"info","ts":"2026-01-18T14:12:58.817972Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3236113679,"revision":14506,"compact-revision":14265}
{"level":"info","ts":"2026-01-18T14:13:45.242405Z","caller":"traceutil/trace.go:171","msg":"trace[1794189070] transaction","detail":"{read_only:false; response_revision:14868; number_of_response:1; }","duration":"180.623247ms","start":"2026-01-18T14:13:45.061764Z","end":"2026-01-18T14:13:45.242387Z","steps":["trace[1794189070] 'process raft request'  (duration: 180.474303ms)"],"step_count":1}
{"level":"info","ts":"2026-01-18T14:17:45.813941Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14831}
{"level":"info","ts":"2026-01-18T14:17:45.817986Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":14831,"took":"3.67818ms","hash":1257795088,"current-db-size-bytes":8245248,"current-db-size":"8.2 MB","current-db-size-in-use-bytes":5087232,"current-db-size-in-use":"5.1 MB"}
{"level":"info","ts":"2026-01-18T14:17:45.818032Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1257795088,"revision":14831,"compact-revision":14506}
{"level":"info","ts":"2026-01-18T14:22:32.962269Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15198}
{"level":"info","ts":"2026-01-18T14:22:32.966755Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":15198,"took":"4.083239ms","hash":1893700413,"current-db-size-bytes":8245248,"current-db-size":"8.2 MB","current-db-size-in-use-bytes":4505600,"current-db-size-in-use":"4.5 MB"}
{"level":"info","ts":"2026-01-18T14:22:32.966809Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1893700413,"revision":15198,"compact-revision":14831}
{"level":"info","ts":"2026-01-18T14:24:55.128551Z","caller":"traceutil/trace.go:171","msg":"trace[1461967708] transaction","detail":"{read_only:false; response_revision:15696; number_of_response:1; }","duration":"104.969036ms","start":"2026-01-18T14:24:55.023561Z","end":"2026-01-18T14:24:55.128530Z","steps":["trace[1461967708] 'process raft request'  (duration: 104.926847ms)"],"step_count":1}
{"level":"info","ts":"2026-01-18T14:24:55.128901Z","caller":"traceutil/trace.go:171","msg":"trace[1342561668] transaction","detail":"{read_only:false; response_revision:15695; number_of_response:1; }","duration":"105.34807ms","start":"2026-01-18T14:24:55.023536Z","end":"2026-01-18T14:24:55.128884Z","steps":["trace[1342561668] 'process raft request'  (duration: 104.862264ms)"],"step_count":1}
{"level":"info","ts":"2026-01-18T14:27:20.217824Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15481}
{"level":"info","ts":"2026-01-18T14:27:20.235683Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":15481,"took":"17.343377ms","hash":3084642221,"current-db-size-bytes":8245248,"current-db-size":"8.2 MB","current-db-size-in-use-bytes":4440064,"current-db-size-in-use":"4.4 MB"}
{"level":"info","ts":"2026-01-18T14:27:20.235752Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3084642221,"revision":15481,"compact-revision":15198}
{"level":"info","ts":"2026-01-18T14:31:39.319665Z","caller":"traceutil/trace.go:171","msg":"trace[824965706] transaction","detail":"{read_only:false; response_revision:16109; number_of_response:1; }","duration":"136.22152ms","start":"2026-01-18T14:31:39.183393Z","end":"2026-01-18T14:31:39.319614Z","steps":["trace[824965706] 'process raft request'  (duration: 136.092733ms)"],"step_count":1}
{"level":"info","ts":"2026-01-18T14:32:07.577001Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15860}
{"level":"info","ts":"2026-01-18T14:32:07.583086Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":15860,"took":"5.640252ms","hash":3620727428,"current-db-size-bytes":8245248,"current-db-size":"8.2 MB","current-db-size-in-use-bytes":4636672,"current-db-size-in-use":"4.6 MB"}
{"level":"info","ts":"2026-01-18T14:32:07.583191Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3620727428,"revision":15860,"compact-revision":15481}
{"level":"info","ts":"2026-01-18T14:34:16.568257Z","caller":"etcdserver/server.go:1473","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":20002,"local-member-snapshot-index":10001,"local-member-snapshot-count":10000}
{"level":"info","ts":"2026-01-18T14:34:16.633371Z","caller":"etcdserver/server.go:2493","msg":"saved snapshot","snapshot-index":20002}
{"level":"info","ts":"2026-01-18T14:34:16.633793Z","caller":"etcdserver/server.go:2523","msg":"compacted Raft logs","compact-index":15002}
{"level":"info","ts":"2026-01-18T14:36:55.017593Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16132}
{"level":"info","ts":"2026-01-18T14:36:55.024588Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":16132,"took":"6.564553ms","hash":2273032686,"current-db-size-bytes":8245248,"current-db-size":"8.2 MB","current-db-size-in-use-bytes":4681728,"current-db-size-in-use":"4.7 MB"}
{"level":"info","ts":"2026-01-18T14:36:55.024651Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2273032686,"revision":16132,"compact-revision":15860}
{"level":"info","ts":"2026-01-18T14:39:26.511892Z","caller":"traceutil/trace.go:171","msg":"trace[844915702] transaction","detail":"{read_only:false; response_revision:16697; number_of_response:1; }","duration":"159.06968ms","start":"2026-01-18T14:39:26.352794Z","end":"2026-01-18T14:39:26.511863Z","steps":["trace[844915702] 'process raft request'  (duration: 158.879734ms)"],"step_count":1}
{"level":"info","ts":"2026-01-18T14:39:41.419733Z","caller":"traceutil/trace.go:171","msg":"trace[285568783] transaction","detail":"{read_only:false; response_revision:16713; number_of_response:1; }","duration":"129.068213ms","start":"2026-01-18T14:39:41.290650Z","end":"2026-01-18T14:39:41.419719Z","steps":["trace[285568783] 'process raft request'  (duration: 128.95441ms)"],"step_count":1}
{"level":"info","ts":"2026-01-18T14:39:41.422041Z","caller":"traceutil/trace.go:171","msg":"trace[1380083886] transaction","detail":"{read_only:false; response_revision:16714; number_of_response:1; }","duration":"130.888068ms","start":"2026-01-18T14:39:41.291143Z","end":"2026-01-18T14:39:41.422032Z","steps":["trace[1380083886] 'process raft request'  (duration: 130.833244ms)"],"step_count":1}
{"level":"info","ts":"2026-01-18T14:39:46.735761Z","caller":"traceutil/trace.go:171","msg":"trace[1105401765] transaction","detail":"{read_only:false; response_revision:16718; number_of_response:1; }","duration":"100.01323ms","start":"2026-01-18T14:39:46.635729Z","end":"2026-01-18T14:39:46.735742Z","steps":["trace[1105401765] 'process raft request'  (duration: 99.659134ms)"],"step_count":1}
{"level":"info","ts":"2026-01-18T14:41:42.560236Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16517}
{"level":"info","ts":"2026-01-18T14:41:42.564251Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":16517,"took":"3.597386ms","hash":2240280735,"current-db-size-bytes":8245248,"current-db-size":"8.2 MB","current-db-size-in-use-bytes":5574656,"current-db-size-in-use":"5.6 MB"}
{"level":"info","ts":"2026-01-18T14:41:42.564296Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2240280735,"revision":16517,"compact-revision":16132}
{"level":"info","ts":"2026-01-18T14:43:28.620054Z","caller":"traceutil/trace.go:171","msg":"trace[963220561] transaction","detail":"{read_only:false; response_revision:17021; number_of_response:1; }","duration":"263.603355ms","start":"2026-01-18T14:43:28.356432Z","end":"2026-01-18T14:43:28.620035Z","steps":["trace[963220561] 'process raft request'  (duration: 263.439605ms)"],"step_count":1}
{"level":"info","ts":"2026-01-18T14:46:30.214237Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16910}
{"level":"info","ts":"2026-01-18T14:46:30.227098Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":16910,"took":"12.488619ms","hash":3587038629,"current-db-size-bytes":8245248,"current-db-size":"8.2 MB","current-db-size-in-use-bytes":3088384,"current-db-size-in-use":"3.1 MB"}
{"level":"info","ts":"2026-01-18T14:46:30.227158Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3587038629,"revision":16910,"compact-revision":16517}
{"level":"info","ts":"2026-01-18T14:46:40.209940Z","caller":"traceutil/trace.go:171","msg":"trace[1246297499] transaction","detail":"{read_only:false; response_revision:17238; number_of_response:1; }","duration":"211.604816ms","start":"2026-01-18T14:46:39.998319Z","end":"2026-01-18T14:46:40.209924Z","steps":["trace[1246297499] 'process raft request'  (duration: 211.379714ms)"],"step_count":1}
{"level":"info","ts":"2026-01-18T14:48:23.688286Z","caller":"traceutil/trace.go:171","msg":"trace[1451636696] linearizableReadLoop","detail":"{readStateIndex:21267; appliedIndex:21266; }","duration":"104.700043ms","start":"2026-01-18T14:48:23.583472Z","end":"2026-01-18T14:48:23.688172Z","steps":["trace[1451636696] 'read index received'  (duration: 102.374017ms)","trace[1451636696] 'applied index is now lower than readState.Index'  (duration: 2.325141ms)"],"step_count":2}
{"level":"warn","ts":"2026-01-18T14:48:23.688888Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.981887ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/\" range_end:\"/registry/events0\" count_only:true ","response":"range_response_count:0 size:9"}
{"level":"info","ts":"2026-01-18T14:48:23.688983Z","caller":"traceutil/trace.go:171","msg":"trace[1555776471] range","detail":"{range_begin:/registry/events/; range_end:/registry/events0; response_count:0; response_revision:17349; }","duration":"105.522812ms","start":"2026-01-18T14:48:23.583438Z","end":"2026-01-18T14:48:23.688961Z","steps":["trace[1555776471] 'agreement among raft nodes before linearized reading'  (duration: 104.850716ms)"],"step_count":1}
{"level":"info","ts":"2026-01-18T14:51:17.943714Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17226}
{"level":"info","ts":"2026-01-18T14:51:17.952109Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":17226,"took":"7.90823ms","hash":1630744802,"current-db-size-bytes":8245248,"current-db-size":"8.2 MB","current-db-size-in-use-bytes":3792896,"current-db-size-in-use":"3.8 MB"}
{"level":"info","ts":"2026-01-18T14:51:17.952167Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1630744802,"revision":17226,"compact-revision":16910}


==> kernel <==
 14:53:47 up  5:18,  0 users,  load average: 0.05, 0.14, 0.20
Linux minikube 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [a11727c2cc11] <==
I0118 10:09:43.369977       1 dynamic_cafile_content.go:175] "Shutting down controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0118 10:09:43.369990       1 dynamic_serving_content.go:149] "Shutting down controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0118 10:09:43.370024       1 secure_serving.go:258] Stopped listening on [::]:8443
W0118 10:09:43.370144       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:43.371515       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
I0118 10:09:43.373019       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
W0118 10:09:44.370870       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.371082       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.371234       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.371263       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.371324       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.371406       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.371485       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.371582       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.371600       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.371726       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.371737       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.371911       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.371942       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.371988       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.371983       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372116       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372161       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372225       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372286       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372175       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372355       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372342       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372403       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372426       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372470       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372325       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372533       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372644       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372653       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372527       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372622       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372299       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372761       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372777       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372795       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372841       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372909       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372927       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372983       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.373044       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.373063       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.373075       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372954       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.373160       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.373154       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.373188       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.373164       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.373219       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.373023       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.372980       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.373166       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.373390       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.373565       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0118 10:09:44.374429       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [fe61d4458742] <==
I0118 10:10:16.106423       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0118 10:10:16.108745       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0118 10:10:16.108978       1 policy_source.go:240] refreshing policies
I0118 10:10:16.109507       1 shared_informer.go:320] Caches are synced for node_authorizer
I0118 10:10:16.109534       1 shared_informer.go:320] Caches are synced for configmaps
I0118 10:10:16.109588       1 cache.go:39] Caches are synced for LocalAvailability controller
I0118 10:10:16.109725       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0118 10:10:16.109744       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0118 10:10:16.109769       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0118 10:10:16.109792       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0118 10:10:16.109773       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0118 10:10:16.109805       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0118 10:10:16.109817       1 aggregator.go:171] initial CRD sync complete...
I0118 10:10:16.109825       1 autoregister_controller.go:144] Starting autoregister controller
I0118 10:10:16.109829       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0118 10:10:16.109833       1 cache.go:39] Caches are synced for autoregister controller
I0118 10:10:16.116626       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
E0118 10:10:16.215578       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0118 10:10:16.706137       1 controller.go:615] quota admission added evaluator for: endpoints
I0118 10:10:16.925600       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0118 10:10:17.006344       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0118 10:10:19.810929       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0118 10:10:19.819561       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0118 10:10:19.819561       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0118 10:10:19.909874       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0118 10:10:20.009323       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0118 10:12:26.596173       1 controller.go:615] quota admission added evaluator for: namespaces
I0118 10:12:30.153305       1 handler.go:286] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0118 10:12:30.675930       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0118 10:12:30.782646       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0118 10:12:31.490625       1 alloc.go:330] "allocated clusterIPs" service="argocd/argocd-applicationset-controller" clusterIPs={"IPv4":"10.99.235.138"}
I0118 10:12:31.560846       1 alloc.go:330] "allocated clusterIPs" service="argocd/argocd-dex-server" clusterIPs={"IPv4":"10.111.124.230"}
I0118 10:12:31.742896       1 alloc.go:330] "allocated clusterIPs" service="argocd/argocd-metrics" clusterIPs={"IPv4":"10.111.8.74"}
I0118 10:12:31.764742       1 alloc.go:330] "allocated clusterIPs" service="argocd/argocd-notifications-controller-metrics" clusterIPs={"IPv4":"10.101.105.140"}
I0118 10:12:31.856420       1 alloc.go:330] "allocated clusterIPs" service="argocd/argocd-redis" clusterIPs={"IPv4":"10.102.173.154"}
I0118 10:12:31.878241       1 alloc.go:330] "allocated clusterIPs" service="argocd/argocd-repo-server" clusterIPs={"IPv4":"10.101.77.69"}
I0118 10:12:32.044347       1 alloc.go:330] "allocated clusterIPs" service="argocd/argocd-server" clusterIPs={"IPv4":"10.98.226.110"}
I0118 10:12:32.142133       1 handler.go:286] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I0118 10:12:32.147975       1 alloc.go:330] "allocated clusterIPs" service="argocd/argocd-server-metrics" clusterIPs={"IPv4":"10.97.100.144"}
I0118 10:12:32.851152       1 controller.go:615] quota admission added evaluator for: statefulsets.apps
I0118 10:12:32.957597       1 controller.go:615] quota admission added evaluator for: networkpolicies.networking.k8s.io
I0118 10:12:33.131997       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0118 10:13:51.791237       1 controller.go:615] quota admission added evaluator for: appprojects.argoproj.io
I0118 12:58:50.429171       1 controller.go:615] quota admission added evaluator for: applications.argoproj.io
I0118 12:58:51.984898       1 alloc.go:330] "allocated clusterIPs" service="12000000-0000-0000-0000-000000000000/ibraheem122-guestbook-helm-guestbook" clusterIPs={"IPv4":"10.106.230.150"}
I0118 13:03:31.393211       1 alloc.go:330] "allocated clusterIPs" service="12000000-0000-0000-0000-000000000000/ibraheem121-guestbook-helm-guestbook" clusterIPs={"IPv4":"10.100.203.202"}
I0118 13:08:29.767786       1 alloc.go:330] "allocated clusterIPs" service="12000000-0000-0000-0000-000000000000/ibraheem120-guestbook-helm-guestbook" clusterIPs={"IPv4":"10.97.33.158"}
I0118 13:39:27.211607       1 alloc.go:330] "allocated clusterIPs" service="12000000-0000-0000-0000-000000000000/ibraheem101-guestbook" clusterIPs={"IPv4":"10.102.214.133"}
I0118 13:39:27.218347       1 alloc.go:330] "allocated clusterIPs" service="12000000-0000-0000-0000-000000000000/redis-master" clusterIPs={"IPv4":"10.107.170.43"}
I0118 13:39:27.221966       1 alloc.go:330] "allocated clusterIPs" service="12000000-0000-0000-0000-000000000000/redis-slave" clusterIPs={"IPv4":"10.98.117.238"}
I0118 13:45:56.420249       1 alloc.go:330] "allocated clusterIPs" service="12000000-0000-0000-0000-000000000000/ibraheem102-guestbook-helm-guestbook" clusterIPs={"IPv4":"10.109.7.5"}
I0118 13:57:16.006183       1 alloc.go:330] "allocated clusterIPs" service="12000000-0000-0000-0000-000000000000/ibraheem102-guestbook-openspeedtest" clusterIPs={"IPv4":"10.104.166.13"}
E0118 14:01:51.667132       1 upgradeaware.go:441] Error proxying data from backend to client: websocket: close sent
I0118 14:10:27.440118       1 controller.go:615] quota admission added evaluator for: poddisruptionbudgets.policy
I0118 14:10:27.475726       1 alloc.go:330] "allocated clusterIPs" service="default/my-rabbitmq" clusterIPs={"IPv4":"10.111.200.61"}
I0118 14:16:47.311108       1 alloc.go:330] "allocated clusterIPs" service="12000000-0000-0000-0000-000000000000/ibraheem102-guestbook-rabbitmq" clusterIPs={"IPv4":"10.110.214.227"}
I0118 14:24:54.847035       1 alloc.go:330] "allocated clusterIPs" service="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-rabbitmq" clusterIPs={"IPv4":"10.102.170.11"}
I0118 14:34:16.063726       1 alloc.go:330] "allocated clusterIPs" service="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api" clusterIPs={"IPv4":"10.100.185.201"}
I0118 14:34:50.014177       1 alloc.go:330] "allocated clusterIPs" service="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api" clusterIPs={"IPv4":"10.98.243.26"}
I0118 14:40:40.541498       1 alloc.go:330] "allocated clusterIPs" service="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api" clusterIPs={"IPv4":"10.100.19.26"}


==> kube-controller-manager [405586ba8e52] <==
I0118 14:34:50.467917       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98" duration="4.592828ms"
E0118 14:34:50.467956       1 replica_set.go:560] "Unhandled Error" err="sync \"12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98\" failed with pods \"ibraheem103-guestbook-weather-api-bddbc5f98-\" is forbidden: error looking up service account 12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api: serviceaccount \"ibraheem103-guestbook-weather-api\" not found" logger="UnhandledError"
I0118 14:34:50.793948       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98" duration="4.097678ms"
E0118 14:34:50.794001       1 replica_set.go:560] "Unhandled Error" err="sync \"12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98\" failed with pods \"ibraheem103-guestbook-weather-api-bddbc5f98-\" is forbidden: error looking up service account 12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api: serviceaccount \"ibraheem103-guestbook-weather-api\" not found" logger="UnhandledError"
I0118 14:34:51.439602       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98" duration="4.147403ms"
E0118 14:34:51.439644       1 replica_set.go:560] "Unhandled Error" err="sync \"12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98\" failed with pods \"ibraheem103-guestbook-weather-api-bddbc5f98-\" is forbidden: error looking up service account 12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api: serviceaccount \"ibraheem103-guestbook-weather-api\" not found" logger="UnhandledError"
I0118 14:34:52.726961       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98" duration="4.908012ms"
E0118 14:34:52.727023       1 replica_set.go:560] "Unhandled Error" err="sync \"12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98\" failed with pods \"ibraheem103-guestbook-weather-api-bddbc5f98-\" is forbidden: error looking up service account 12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api: serviceaccount \"ibraheem103-guestbook-weather-api\" not found" logger="UnhandledError"
I0118 14:34:55.292870       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98" duration="3.920483ms"
E0118 14:34:55.292910       1 replica_set.go:560] "Unhandled Error" err="sync \"12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98\" failed with pods \"ibraheem103-guestbook-weather-api-bddbc5f98-\" is forbidden: error looking up service account 12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api: serviceaccount \"ibraheem103-guestbook-weather-api\" not found" logger="UnhandledError"
I0118 14:34:59.133176       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98" duration="4.443533ms"
E0118 14:34:59.133229       1 replica_set.go:560] "Unhandled Error" err="sync \"12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98\" failed with pods \"ibraheem103-guestbook-weather-api-bddbc5f98-\" is forbidden: error looking up service account 12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api: serviceaccount \"ibraheem103-guestbook-weather-api\" not found" logger="UnhandledError"
I0118 14:35:09.379101       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98" duration="4.156655ms"
E0118 14:35:09.379143       1 replica_set.go:560] "Unhandled Error" err="sync \"12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98\" failed with pods \"ibraheem103-guestbook-weather-api-bddbc5f98-\" is forbidden: error looking up service account 12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api: serviceaccount \"ibraheem103-guestbook-weather-api\" not found" logger="UnhandledError"
I0118 14:35:28.620307       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98" duration="6.327024ms"
E0118 14:35:28.620348       1 replica_set.go:560] "Unhandled Error" err="sync \"12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98\" failed with pods \"ibraheem103-guestbook-weather-api-bddbc5f98-\" is forbidden: error looking up service account 12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api: serviceaccount \"ibraheem103-guestbook-weather-api\" not found" logger="UnhandledError"
I0118 14:36:08.338613       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98" duration="5.993575ms"
E0118 14:36:08.338651       1 replica_set.go:560] "Unhandled Error" err="sync \"12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98\" failed with pods \"ibraheem103-guestbook-weather-api-bddbc5f98-\" is forbidden: error looking up service account 12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api: serviceaccount \"ibraheem103-guestbook-weather-api\" not found" logger="UnhandledError"
I0118 14:37:02.158296       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0118 14:37:26.521342       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98" duration="5.906539ms"
E0118 14:37:26.521389       1 replica_set.go:560] "Unhandled Error" err="sync \"12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98\" failed with pods \"ibraheem103-guestbook-weather-api-bddbc5f98-\" is forbidden: error looking up service account 12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api: serviceaccount \"ibraheem103-guestbook-weather-api\" not found" logger="UnhandledError"
I0118 14:39:09.755931       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="21.401971ms"
I0118 14:39:09.817588       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="61.56758ms"
I0118 14:39:09.817720       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="42.948s"
I0118 14:39:12.599343       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="95.346s"
I0118 14:39:24.300119       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="50.543s"
I0118 14:39:41.424012       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="98.097s"
I0118 14:39:54.076695       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="124.483s"
I0118 14:40:04.151100       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98" duration="6.557449ms"
E0118 14:40:04.151152       1 replica_set.go:560] "Unhandled Error" err="sync \"12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98\" failed with pods \"ibraheem103-guestbook-weather-api-bddbc5f98-\" is forbidden: error looking up service account 12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api: serviceaccount \"ibraheem103-guestbook-weather-api\" not found" logger="UnhandledError"
I0118 14:40:08.064687       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="55.945s"
I0118 14:40:19.818678       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="79.328s"
I0118 14:40:40.557354       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98" duration="15.176s"
I0118 14:40:40.589515       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="11.919s"
I0118 14:40:40.701961       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="100.74305ms"
I0118 14:40:40.720127       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="18.097004ms"
I0118 14:40:40.720250       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="50.636s"
I0118 14:40:43.870415       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="52.882s"
I0118 14:40:57.569299       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="45.52s"
I0118 14:41:12.561939       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="67.101s"
I0118 14:41:25.324655       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="49.184s"
I0118 14:41:43.077867       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="56.996s"
I0118 14:41:54.077688       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="92.104s"
I0118 14:41:56.157524       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0118 14:42:22.844317       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="81.418s"
I0118 14:42:37.842936       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="94.341s"
I0118 14:43:40.117817       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="129.444s"
I0118 14:43:54.130640       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="90.485s"
I0118 14:45:18.195302       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-bddbc5f98" duration="25.352s"
I0118 14:46:17.964841       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="96.761s"
I0118 14:46:19.609674       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc" duration="17.928517ms"
I0118 14:46:19.640490       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc" duration="30.758605ms"
I0118 14:46:19.640636       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc" duration="69.647s"
I0118 14:46:19.644996       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc" duration="71.374s"
I0118 14:46:20.493898       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc" duration="43.553s"
I0118 14:46:31.714423       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="112.564s"
I0118 14:46:49.608182       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0118 14:51:17.461509       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="94.492s"
I0118 14:51:28.463112       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8" duration="118.543s"
I0118 14:51:44.400139       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-controller-manager [aa464cc92e4b] <==
I0118 09:52:50.042331       1 shared_informer.go:320] Caches are synced for disruption
I0118 09:52:50.056579       1 shared_informer.go:320] Caches are synced for HPA
I0118 09:52:50.056647       1 shared_informer.go:320] Caches are synced for garbage collector
I0118 09:52:50.056667       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0118 09:52:50.056675       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0118 09:52:50.057235       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0118 09:52:50.058176       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0118 09:52:50.058405       1 shared_informer.go:320] Caches are synced for stateful set
I0118 09:52:50.058432       1 shared_informer.go:320] Caches are synced for expand
I0118 09:52:50.058446       1 shared_informer.go:320] Caches are synced for attach detach
I0118 09:52:50.058457       1 shared_informer.go:320] Caches are synced for node
I0118 09:52:50.058497       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0118 09:52:50.058555       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0118 09:52:50.058582       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0118 09:52:50.058591       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0118 09:52:50.058698       1 shared_informer.go:320] Caches are synced for cidrallocator
I0118 09:52:50.059129       1 shared_informer.go:320] Caches are synced for taint
I0118 09:52:50.059288       1 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0118 09:52:50.059348       1 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0118 09:52:50.059404       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0118 09:52:50.060176       1 shared_informer.go:320] Caches are synced for endpoint
I0118 09:52:50.061841       1 shared_informer.go:320] Caches are synced for namespace
I0118 09:52:50.064199       1 shared_informer.go:320] Caches are synced for resource quota
I0118 09:52:50.066499       1 shared_informer.go:320] Caches are synced for resource quota
I0118 09:52:50.067458       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0118 09:52:50.067498       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0118 09:52:50.067529       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0118 09:52:50.073483       1 shared_informer.go:320] Caches are synced for garbage collector
I0118 09:52:50.260507       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0118 09:52:50.712309       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0118 09:52:51.178984       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="413.119771ms"
I0118 09:52:51.178984       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="415.748924ms"
I0118 09:52:51.183940       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="418.171062ms"
I0118 09:52:51.203774       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="24.716893ms"
I0118 09:52:51.203803       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="19.830472ms"
I0118 09:52:51.203881       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="24.792821ms"
I0118 09:52:51.204026       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="77.716s"
I0118 09:52:51.204075       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="27.098s"
I0118 09:52:51.219533       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="15.616979ms"
I0118 09:52:51.219646       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="55.782s"
I0118 09:52:51.220783       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="16.971801ms"
I0118 09:52:51.220876       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="45.517s"
I0118 09:52:51.224567       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="65.676s"
I0118 09:52:51.232143       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="63.674s"
I0118 09:52:53.310261       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="127.677s"
I0118 09:52:53.320414       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="80.023s"
I0118 09:52:55.979175       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0118 09:52:56.802359       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="8.859478ms"
I0118 09:52:56.802439       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="36.307s"
I0118 09:53:00.336532       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="16.343381ms"
I0118 09:53:00.336614       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="38.795s"
I0118 09:53:01.727974       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="6.904918ms"
I0118 09:53:01.728101       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="32.416s"
I0118 09:53:14.012024       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0118 09:53:34.320255       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="7.100453ms"
I0118 09:53:34.320374       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="42.042s"
I0118 09:53:44.573846       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0118 09:57:50.886351       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0118 10:02:45.301958       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0118 10:07:38.457675       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [f8c6a5e5c3d0] <==
I0118 10:10:19.424871       1 server_linux.go:66] "Using iptables proxy"
I0118 10:10:19.914811       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0118 10:10:19.914906       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0118 10:10:20.021119       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0118 10:10:20.021173       1 server_linux.go:170] "Using iptables Proxier"
I0118 10:10:20.023279       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0118 10:10:20.023876       1 server.go:497] "Version info" version="v1.32.0"
I0118 10:10:20.023894       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0118 10:10:20.025364       1 config.go:329] "Starting node config controller"
I0118 10:10:20.025396       1 shared_informer.go:313] Waiting for caches to sync for node config
I0118 10:10:20.025691       1 config.go:199] "Starting service config controller"
I0118 10:10:20.025701       1 shared_informer.go:313] Waiting for caches to sync for service config
I0118 10:10:20.025715       1 config.go:105] "Starting endpoint slice config controller"
I0118 10:10:20.025726       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0118 10:10:20.125994       1 shared_informer.go:320] Caches are synced for node config
I0118 10:10:20.202329       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0118 10:10:20.202782       1 shared_informer.go:320] Caches are synced for service config


==> kube-proxy [fff7680f5566] <==
I0118 09:52:51.353587       1 server_linux.go:66] "Using iptables proxy"
I0118 09:52:51.498265       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0118 09:52:51.498537       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0118 09:52:51.523126       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0118 09:52:51.523179       1 server_linux.go:170] "Using iptables Proxier"
I0118 09:52:51.525459       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0118 09:52:51.529116       1 server.go:497] "Version info" version="v1.32.0"
I0118 09:52:51.529150       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0118 09:52:51.531779       1 config.go:199] "Starting service config controller"
I0118 09:52:51.531807       1 config.go:105] "Starting endpoint slice config controller"
I0118 09:52:51.531833       1 shared_informer.go:313] Waiting for caches to sync for service config
I0118 09:52:51.531834       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0118 09:52:51.531870       1 config.go:329] "Starting node config controller"
I0118 09:52:51.531875       1 shared_informer.go:313] Waiting for caches to sync for node config
I0118 09:52:51.632450       1 shared_informer.go:320] Caches are synced for node config
I0118 09:52:51.632476       1 shared_informer.go:320] Caches are synced for service config
I0118 09:52:51.632495       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [98c23fd63bbc] <==
I0118 09:52:42.252245       1 serving.go:386] Generated self-signed cert in-memory
W0118 09:52:43.400089       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0118 09:52:43.400120       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0118 09:52:43.400130       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0118 09:52:43.400137       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0118 09:52:43.593094       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0118 09:52:43.593163       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0118 09:52:43.596930       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0118 09:52:43.597362       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0118 09:52:43.597683       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0118 09:52:43.598065       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
W0118 09:52:43.599066       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0118 09:52:43.599124       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0118 09:52:43.599212       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0118 09:52:43.599240       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0118 09:52:43.599256       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0118 09:52:43.599286       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0118 09:52:43.599298       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0118 09:52:43.599368       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0118 09:52:43.599427       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0118 09:52:43.599461       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0118 09:52:43.599469       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0118 09:52:43.599471       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0118 09:52:43.599559       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0118 09:52:43.599582       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0118 09:52:43.599650       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0118 09:52:43.599677       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
E0118 09:52:43.599691       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0118 09:52:43.599591       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0118 09:52:43.599751       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0118 09:52:43.599721       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0118 09:52:43.599777       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0118 09:52:43.599779       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0118 09:52:43.599796       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0118 09:52:43.599833       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0118 09:52:43.599861       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0118 09:52:43.599877       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0118 09:52:43.599881       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0118 09:52:43.599597       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0118 09:52:43.599977       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0118 09:52:43.600008       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0118 09:52:43.599988       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0118 09:52:43.600059       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0118 09:52:44.420571       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0118 09:52:44.420615       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0118 09:52:44.473555       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0118 09:52:44.473595       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0118 09:52:44.482983       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0118 09:52:44.483041       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0118 09:52:44.525080       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0118 09:52:44.525124       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0118 09:52:44.531938       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0118 09:52:44.531992       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
I0118 09:52:47.698459       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0118 10:09:43.367147       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I0118 10:09:43.367390       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0118 10:09:43.371565       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0118 10:09:43.376152       1 run.go:72] "command failed" err="finished without leader elect"


==> kube-scheduler [a2be7b52b292] <==
I0118 10:10:14.225276       1 serving.go:386] Generated self-signed cert in-memory
W0118 10:10:16.107904       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0118 10:10:16.108815       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0118 10:10:16.109182       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0118 10:10:16.109342       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0118 10:10:16.119679       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0118 10:10:16.119719       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0118 10:10:16.121109       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0118 10:10:16.121262       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0118 10:10:16.121306       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0118 10:10:16.121316       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0118 10:10:16.303187       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Jan 18 14:49:54 minikube kubelet[1523]: E0118 14:49:54.100564    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ImagePullBackOff: \"Back-off pulling image \\\"weatherapi:latest\\\": ErrImagePull: Error response from daemon: pull access denied for weatherapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8-v4hn4" podUID="6e2de123-23ed-42c3-b070-d946ddec7a45"
Jan 18 14:50:00 minikube kubelet[1523]: E0118 14:50:00.099846    1523 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:weather-api,Image:weatherapi:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc5pt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h_12000000-0000-0000-0000-000000000000(54d73654-b1fd-4505-93fb-34f739fc40a8): ErrImageNeverPull: Container image \"weatherapi:latest\" is not present with pull policy of Never" logger="UnhandledError"
Jan 18 14:50:00 minikube kubelet[1523]: E0118 14:50:00.101075    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ErrImageNeverPull: \"Container image \\\"weatherapi:latest\\\" is not present with pull policy of Never\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h" podUID="54d73654-b1fd-4505-93fb-34f739fc40a8"
Jan 18 14:50:09 minikube kubelet[1523]: E0118 14:50:09.100026    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ImagePullBackOff: \"Back-off pulling image \\\"weatherapi:latest\\\": ErrImagePull: Error response from daemon: pull access denied for weatherapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8-v4hn4" podUID="6e2de123-23ed-42c3-b070-d946ddec7a45"
Jan 18 14:50:11 minikube kubelet[1523]: E0118 14:50:11.101997    1523 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:weather-api,Image:weatherapi:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc5pt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h_12000000-0000-0000-0000-000000000000(54d73654-b1fd-4505-93fb-34f739fc40a8): ErrImageNeverPull: Container image \"weatherapi:latest\" is not present with pull policy of Never" logger="UnhandledError"
Jan 18 14:50:11 minikube kubelet[1523]: E0118 14:50:11.103210    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ErrImageNeverPull: \"Container image \\\"weatherapi:latest\\\" is not present with pull policy of Never\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h" podUID="54d73654-b1fd-4505-93fb-34f739fc40a8"
Jan 18 14:50:19 minikube kubelet[1523]: E0118 14:50:19.882224    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ImagePullBackOff: \"Back-off pulling image \\\"weatherapi:latest\\\": ErrImagePull: Error response from daemon: pull access denied for weatherapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8-v4hn4" podUID="6e2de123-23ed-42c3-b070-d946ddec7a45"
Jan 18 14:50:21 minikube kubelet[1523]: E0118 14:50:21.882848    1523 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:weather-api,Image:weatherapi:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc5pt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h_12000000-0000-0000-0000-000000000000(54d73654-b1fd-4505-93fb-34f739fc40a8): ErrImageNeverPull: Container image \"weatherapi:latest\" is not present with pull policy of Never" logger="UnhandledError"
Jan 18 14:50:21 minikube kubelet[1523]: E0118 14:50:21.884020    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ErrImageNeverPull: \"Container image \\\"weatherapi:latest\\\" is not present with pull policy of Never\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h" podUID="54d73654-b1fd-4505-93fb-34f739fc40a8"
Jan 18 14:50:34 minikube kubelet[1523]: E0118 14:50:34.882265    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ImagePullBackOff: \"Back-off pulling image \\\"weatherapi:latest\\\": ErrImagePull: Error response from daemon: pull access denied for weatherapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8-v4hn4" podUID="6e2de123-23ed-42c3-b070-d946ddec7a45"
Jan 18 14:50:35 minikube kubelet[1523]: E0118 14:50:35.882298    1523 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:weather-api,Image:weatherapi:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc5pt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h_12000000-0000-0000-0000-000000000000(54d73654-b1fd-4505-93fb-34f739fc40a8): ErrImageNeverPull: Container image \"weatherapi:latest\" is not present with pull policy of Never" logger="UnhandledError"
Jan 18 14:50:35 minikube kubelet[1523]: E0118 14:50:35.883503    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ErrImageNeverPull: \"Container image \\\"weatherapi:latest\\\" is not present with pull policy of Never\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h" podUID="54d73654-b1fd-4505-93fb-34f739fc40a8"
Jan 18 14:50:48 minikube kubelet[1523]: E0118 14:50:48.663113    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ImagePullBackOff: \"Back-off pulling image \\\"weatherapi:latest\\\": ErrImagePull: Error response from daemon: pull access denied for weatherapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8-v4hn4" podUID="6e2de123-23ed-42c3-b070-d946ddec7a45"
Jan 18 14:50:49 minikube kubelet[1523]: E0118 14:50:49.662937    1523 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:weather-api,Image:weatherapi:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc5pt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h_12000000-0000-0000-0000-000000000000(54d73654-b1fd-4505-93fb-34f739fc40a8): ErrImageNeverPull: Container image \"weatherapi:latest\" is not present with pull policy of Never" logger="UnhandledError"
Jan 18 14:50:49 minikube kubelet[1523]: E0118 14:50:49.664158    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ErrImageNeverPull: \"Container image \\\"weatherapi:latest\\\" is not present with pull policy of Never\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h" podUID="54d73654-b1fd-4505-93fb-34f739fc40a8"
Jan 18 14:51:01 minikube kubelet[1523]: E0118 14:51:01.664597    1523 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:weather-api,Image:weatherapi:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc5pt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h_12000000-0000-0000-0000-000000000000(54d73654-b1fd-4505-93fb-34f739fc40a8): ErrImageNeverPull: Container image \"weatherapi:latest\" is not present with pull policy of Never" logger="UnhandledError"
Jan 18 14:51:01 minikube kubelet[1523]: E0118 14:51:01.665860    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ErrImageNeverPull: \"Container image \\\"weatherapi:latest\\\" is not present with pull policy of Never\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h" podUID="54d73654-b1fd-4505-93fb-34f739fc40a8"
Jan 18 14:51:05 minikube kubelet[1523]: E0118 14:51:05.742292    1523 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for weatherapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="weatherapi:latest"
Jan 18 14:51:05 minikube kubelet[1523]: E0118 14:51:05.742371    1523 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for weatherapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="weatherapi:latest"
Jan 18 14:51:05 minikube kubelet[1523]: E0118 14:51:05.742479    1523 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:weather-api,Image:weatherapi:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-682bp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibraheem103-guestbook-weather-api-66c9b9bfc8-v4hn4_12000000-0000-0000-0000-000000000000(6e2de123-23ed-42c3-b070-d946ddec7a45): ErrImagePull: Error response from daemon: pull access denied for weatherapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Jan 18 14:51:05 minikube kubelet[1523]: E0118 14:51:05.743722    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ErrImagePull: \"Error response from daemon: pull access denied for weatherapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8-v4hn4" podUID="6e2de123-23ed-42c3-b070-d946ddec7a45"
Jan 18 14:51:15 minikube kubelet[1523]: E0118 14:51:15.663278    1523 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:weather-api,Image:weatherapi:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc5pt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h_12000000-0000-0000-0000-000000000000(54d73654-b1fd-4505-93fb-34f739fc40a8): ErrImageNeverPull: Container image \"weatherapi:latest\" is not present with pull policy of Never" logger="UnhandledError"
Jan 18 14:51:15 minikube kubelet[1523]: E0118 14:51:15.664487    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ErrImageNeverPull: \"Container image \\\"weatherapi:latest\\\" is not present with pull policy of Never\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h" podUID="54d73654-b1fd-4505-93fb-34f739fc40a8"
Jan 18 14:51:17 minikube kubelet[1523]: E0118 14:51:17.449692    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ImagePullBackOff: \"Back-off pulling image \\\"weatherapi:latest\\\": ErrImagePull: Error response from daemon: pull access denied for weatherapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8-v4hn4" podUID="6e2de123-23ed-42c3-b070-d946ddec7a45"
Jan 18 14:51:28 minikube kubelet[1523]: E0118 14:51:28.449187    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ImagePullBackOff: \"Back-off pulling image \\\"weatherapi:latest\\\": ErrImagePull: Error response from daemon: pull access denied for weatherapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8-v4hn4" podUID="6e2de123-23ed-42c3-b070-d946ddec7a45"
Jan 18 14:51:29 minikube kubelet[1523]: E0118 14:51:29.449066    1523 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:weather-api,Image:weatherapi:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc5pt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h_12000000-0000-0000-0000-000000000000(54d73654-b1fd-4505-93fb-34f739fc40a8): ErrImageNeverPull: Container image \"weatherapi:latest\" is not present with pull policy of Never" logger="UnhandledError"
Jan 18 14:51:29 minikube kubelet[1523]: E0118 14:51:29.451208    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ErrImageNeverPull: \"Container image \\\"weatherapi:latest\\\" is not present with pull policy of Never\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h" podUID="54d73654-b1fd-4505-93fb-34f739fc40a8"
Jan 18 14:51:41 minikube kubelet[1523]: E0118 14:51:41.447142    1523 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:weather-api,Image:weatherapi:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc5pt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h_12000000-0000-0000-0000-000000000000(54d73654-b1fd-4505-93fb-34f739fc40a8): ErrImageNeverPull: Container image \"weatherapi:latest\" is not present with pull policy of Never" logger="UnhandledError"
Jan 18 14:51:41 minikube kubelet[1523]: E0118 14:51:41.448961    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ErrImageNeverPull: \"Container image \\\"weatherapi:latest\\\" is not present with pull policy of Never\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h" podUID="54d73654-b1fd-4505-93fb-34f739fc40a8"
Jan 18 14:51:42 minikube kubelet[1523]: E0118 14:51:42.445957    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ImagePullBackOff: \"Back-off pulling image \\\"weatherapi:latest\\\": ErrImagePull: Error response from daemon: pull access denied for weatherapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8-v4hn4" podUID="6e2de123-23ed-42c3-b070-d946ddec7a45"
Jan 18 14:51:54 minikube kubelet[1523]: E0118 14:51:54.216956    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ImagePullBackOff: \"Back-off pulling image \\\"weatherapi:latest\\\": ErrImagePull: Error response from daemon: pull access denied for weatherapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8-v4hn4" podUID="6e2de123-23ed-42c3-b070-d946ddec7a45"
Jan 18 14:51:54 minikube kubelet[1523]: E0118 14:51:54.217000    1523 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:weather-api,Image:weatherapi:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc5pt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h_12000000-0000-0000-0000-000000000000(54d73654-b1fd-4505-93fb-34f739fc40a8): ErrImageNeverPull: Container image \"weatherapi:latest\" is not present with pull policy of Never" logger="UnhandledError"
Jan 18 14:51:54 minikube kubelet[1523]: E0118 14:51:54.218215    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ErrImageNeverPull: \"Container image \\\"weatherapi:latest\\\" is not present with pull policy of Never\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h" podUID="54d73654-b1fd-4505-93fb-34f739fc40a8"
Jan 18 14:52:05 minikube kubelet[1523]: E0118 14:52:05.217118    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ImagePullBackOff: \"Back-off pulling image \\\"weatherapi:latest\\\": ErrImagePull: Error response from daemon: pull access denied for weatherapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8-v4hn4" podUID="6e2de123-23ed-42c3-b070-d946ddec7a45"
Jan 18 14:52:09 minikube kubelet[1523]: E0118 14:52:09.217079    1523 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:weather-api,Image:weatherapi:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc5pt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h_12000000-0000-0000-0000-000000000000(54d73654-b1fd-4505-93fb-34f739fc40a8): ErrImageNeverPull: Container image \"weatherapi:latest\" is not present with pull policy of Never" logger="UnhandledError"
Jan 18 14:52:09 minikube kubelet[1523]: E0118 14:52:09.218358    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ErrImageNeverPull: \"Container image \\\"weatherapi:latest\\\" is not present with pull policy of Never\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h" podUID="54d73654-b1fd-4505-93fb-34f739fc40a8"
Jan 18 14:52:15 minikube kubelet[1523]: E0118 14:52:15.983699    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ImagePullBackOff: \"Back-off pulling image \\\"weatherapi:latest\\\": ErrImagePull: Error response from daemon: pull access denied for weatherapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8-v4hn4" podUID="6e2de123-23ed-42c3-b070-d946ddec7a45"
Jan 18 14:52:18 minikube kubelet[1523]: E0118 14:52:18.983774    1523 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:weather-api,Image:weatherapi:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc5pt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h_12000000-0000-0000-0000-000000000000(54d73654-b1fd-4505-93fb-34f739fc40a8): ErrImageNeverPull: Container image \"weatherapi:latest\" is not present with pull policy of Never" logger="UnhandledError"
Jan 18 14:52:18 minikube kubelet[1523]: E0118 14:52:18.984887    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ErrImageNeverPull: \"Container image \\\"weatherapi:latest\\\" is not present with pull policy of Never\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h" podUID="54d73654-b1fd-4505-93fb-34f739fc40a8"
Jan 18 14:52:26 minikube kubelet[1523]: E0118 14:52:26.983460    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ImagePullBackOff: \"Back-off pulling image \\\"weatherapi:latest\\\": ErrImagePull: Error response from daemon: pull access denied for weatherapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8-v4hn4" podUID="6e2de123-23ed-42c3-b070-d946ddec7a45"
Jan 18 14:52:31 minikube kubelet[1523]: E0118 14:52:31.985625    1523 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:weather-api,Image:weatherapi:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc5pt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h_12000000-0000-0000-0000-000000000000(54d73654-b1fd-4505-93fb-34f739fc40a8): ErrImageNeverPull: Container image \"weatherapi:latest\" is not present with pull policy of Never" logger="UnhandledError"
Jan 18 14:52:31 minikube kubelet[1523]: E0118 14:52:31.987008    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ErrImageNeverPull: \"Container image \\\"weatherapi:latest\\\" is not present with pull policy of Never\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h" podUID="54d73654-b1fd-4505-93fb-34f739fc40a8"
Jan 18 14:52:39 minikube kubelet[1523]: E0118 14:52:39.989363    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ImagePullBackOff: \"Back-off pulling image \\\"weatherapi:latest\\\": ErrImagePull: Error response from daemon: pull access denied for weatherapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8-v4hn4" podUID="6e2de123-23ed-42c3-b070-d946ddec7a45"
Jan 18 14:52:43 minikube kubelet[1523]: E0118 14:52:43.762297    1523 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:weather-api,Image:weatherapi:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc5pt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h_12000000-0000-0000-0000-000000000000(54d73654-b1fd-4505-93fb-34f739fc40a8): ErrImageNeverPull: Container image \"weatherapi:latest\" is not present with pull policy of Never" logger="UnhandledError"
Jan 18 14:52:43 minikube kubelet[1523]: E0118 14:52:43.763799    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ErrImageNeverPull: \"Container image \\\"weatherapi:latest\\\" is not present with pull policy of Never\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h" podUID="54d73654-b1fd-4505-93fb-34f739fc40a8"
Jan 18 14:52:52 minikube kubelet[1523]: E0118 14:52:52.765977    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ImagePullBackOff: \"Back-off pulling image \\\"weatherapi:latest\\\": ErrImagePull: Error response from daemon: pull access denied for weatherapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8-v4hn4" podUID="6e2de123-23ed-42c3-b070-d946ddec7a45"
Jan 18 14:52:56 minikube kubelet[1523]: E0118 14:52:56.764836    1523 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:weather-api,Image:weatherapi:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc5pt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h_12000000-0000-0000-0000-000000000000(54d73654-b1fd-4505-93fb-34f739fc40a8): ErrImageNeverPull: Container image \"weatherapi:latest\" is not present with pull policy of Never" logger="UnhandledError"
Jan 18 14:52:56 minikube kubelet[1523]: E0118 14:52:56.766951    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ErrImageNeverPull: \"Container image \\\"weatherapi:latest\\\" is not present with pull policy of Never\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h" podUID="54d73654-b1fd-4505-93fb-34f739fc40a8"
Jan 18 14:53:05 minikube kubelet[1523]: E0118 14:53:05.761193    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ImagePullBackOff: \"Back-off pulling image \\\"weatherapi:latest\\\": ErrImagePull: Error response from daemon: pull access denied for weatherapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8-v4hn4" podUID="6e2de123-23ed-42c3-b070-d946ddec7a45"
Jan 18 14:53:10 minikube kubelet[1523]: E0118 14:53:10.761281    1523 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:weather-api,Image:weatherapi:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc5pt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h_12000000-0000-0000-0000-000000000000(54d73654-b1fd-4505-93fb-34f739fc40a8): ErrImageNeverPull: Container image \"weatherapi:latest\" is not present with pull policy of Never" logger="UnhandledError"
Jan 18 14:53:10 minikube kubelet[1523]: E0118 14:53:10.762885    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ErrImageNeverPull: \"Container image \\\"weatherapi:latest\\\" is not present with pull policy of Never\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h" podUID="54d73654-b1fd-4505-93fb-34f739fc40a8"
Jan 18 14:53:15 minikube kubelet[1523]: E0118 14:53:15.541524    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ImagePullBackOff: \"Back-off pulling image \\\"weatherapi:latest\\\": ErrImagePull: Error response from daemon: pull access denied for weatherapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8-v4hn4" podUID="6e2de123-23ed-42c3-b070-d946ddec7a45"
Jan 18 14:53:24 minikube kubelet[1523]: E0118 14:53:24.545011    1523 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:weather-api,Image:weatherapi:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc5pt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h_12000000-0000-0000-0000-000000000000(54d73654-b1fd-4505-93fb-34f739fc40a8): ErrImageNeverPull: Container image \"weatherapi:latest\" is not present with pull policy of Never" logger="UnhandledError"
Jan 18 14:53:24 minikube kubelet[1523]: E0118 14:53:24.546897    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ErrImageNeverPull: \"Container image \\\"weatherapi:latest\\\" is not present with pull policy of Never\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h" podUID="54d73654-b1fd-4505-93fb-34f739fc40a8"
Jan 18 14:53:28 minikube kubelet[1523]: E0118 14:53:28.543755    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ImagePullBackOff: \"Back-off pulling image \\\"weatherapi:latest\\\": ErrImagePull: Error response from daemon: pull access denied for weatherapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8-v4hn4" podUID="6e2de123-23ed-42c3-b070-d946ddec7a45"
Jan 18 14:53:37 minikube kubelet[1523]: E0118 14:53:37.541521    1523 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:weather-api,Image:weatherapi:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc5pt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h_12000000-0000-0000-0000-000000000000(54d73654-b1fd-4505-93fb-34f739fc40a8): ErrImageNeverPull: Container image \"weatherapi:latest\" is not present with pull policy of Never" logger="UnhandledError"
Jan 18 14:53:37 minikube kubelet[1523]: E0118 14:53:37.542724    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ErrImageNeverPull: \"Container image \\\"weatherapi:latest\\\" is not present with pull policy of Never\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h" podUID="54d73654-b1fd-4505-93fb-34f739fc40a8"
Jan 18 14:53:39 minikube kubelet[1523]: E0118 14:53:39.542003    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ImagePullBackOff: \"Back-off pulling image \\\"weatherapi:latest\\\": ErrImagePull: Error response from daemon: pull access denied for weatherapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-66c9b9bfc8-v4hn4" podUID="6e2de123-23ed-42c3-b070-d946ddec7a45"
Jan 18 14:53:47 minikube kubelet[1523]: E0118 14:53:47.326750    1523 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:weather-api,Image:weatherapi:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc5pt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,AppArmorProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h_12000000-0000-0000-0000-000000000000(54d73654-b1fd-4505-93fb-34f739fc40a8): ErrImageNeverPull: Container image \"weatherapi:latest\" is not present with pull policy of Never" logger="UnhandledError"
Jan 18 14:53:47 minikube kubelet[1523]: E0118 14:53:47.327993    1523 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"weather-api\" with ErrImageNeverPull: \"Container image \\\"weatherapi:latest\\\" is not present with pull policy of Never\"" pod="12000000-0000-0000-0000-000000000000/ibraheem103-guestbook-weather-api-f9bdc6cdc-k2p9h" podUID="54d73654-b1fd-4505-93fb-34f739fc40a8"


==> kubernetes-dashboard [8699e0eb553e] <==
2026/01/18 10:10:57 Starting overwatch
2026/01/18 10:10:57 Using namespace: kubernetes-dashboard
2026/01/18 10:10:57 Using in-cluster config to connect to apiserver
2026/01/18 10:10:57 Using secret token for csrf signing
2026/01/18 10:10:57 Initializing csrf token from kubernetes-dashboard-csrf secret
2026/01/18 10:10:57 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2026/01/18 10:10:57 Successful initial request to the apiserver, version: v1.32.0
2026/01/18 10:10:57 Generating JWE encryption key
2026/01/18 10:10:57 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2026/01/18 10:10:57 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2026/01/18 10:10:57 Initializing JWE encryption key from synchronized object
2026/01/18 10:10:57 Creating in-cluster Sidecar client
2026/01/18 10:10:57 Serving insecurely on HTTP port: 9090
2026/01/18 10:10:57 Successful request to sidecar


==> kubernetes-dashboard [ba7f20314ca7] <==
2026/01/18 10:10:20 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: connect: connection refused

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc00081fae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc00004a100)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf
2026/01/18 10:10:20 Using namespace: kubernetes-dashboard
2026/01/18 10:10:20 Using in-cluster config to connect to apiserver
2026/01/18 10:10:20 Using secret token for csrf signing
2026/01/18 10:10:20 Initializing csrf token from kubernetes-dashboard-csrf secret


==> storage-provisioner [06e20ae45c90] <==
I0118 10:10:53.436423       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0118 10:10:53.442462       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0118 10:10:53.442510       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0118 10:11:10.845577       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0118 10:11:10.845734       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_00d54c0d-1dd2-4311-b9e3-921d4539f034!
I0118 10:11:10.845967       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"0c141ab5-31e8-45c9-8134-366354416604", APIVersion:"v1", ResourceVersion:"1468", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_00d54c0d-1dd2-4311-b9e3-921d4539f034 became leader
I0118 10:11:10.946837       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_00d54c0d-1dd2-4311-b9e3-921d4539f034!
I0118 14:10:27.545988       1 controller.go:1332] provision "default/data-my-rabbitmq-0" class "standard": started
I0118 14:10:27.547540       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"data-my-rabbitmq-0", UID:"fb41eb6d-188d-4291-bc2b-8e46ff2bcb31", APIVersion:"v1", ResourceVersion:"14645", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/data-my-rabbitmq-0"
I0118 14:10:27.547175       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    eb1b9de1-4277-4e50-9c9e-757e99949424 269 0 2026-01-18 09:52:47 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2026-01-18 09:52:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-fb41eb6d-188d-4291-bc2b-8e46ff2bcb31 &PersistentVolumeClaim{ObjectMeta:{data-my-rabbitmq-0  default  fb41eb6d-188d-4291-bc2b-8e46ff2bcb31 14645 0 2026-01-18 14:10:27 +0000 UTC <nil> <nil> map[app.kubernetes.io/instance:my-rabbitmq app.kubernetes.io/name:rabbitmq] map[volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2026-01-18 14:10:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}},"f:labels":{".":{},"f:app.kubernetes.io/instance":{},"f:app.kubernetes.io/name":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{8589934592 0} {<nil>}  BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/default/data-my-rabbitmq-0
I0118 14:10:27.553081       1 controller.go:1439] provision "default/data-my-rabbitmq-0" class "standard": volume "pvc-fb41eb6d-188d-4291-bc2b-8e46ff2bcb31" provisioned
I0118 14:10:27.553163       1 controller.go:1456] provision "default/data-my-rabbitmq-0" class "standard": succeeded
I0118 14:10:27.553200       1 volume_store.go:212] Trying to save persistentvolume "pvc-fb41eb6d-188d-4291-bc2b-8e46ff2bcb31"
I0118 14:10:27.560413       1 volume_store.go:219] persistentvolume "pvc-fb41eb6d-188d-4291-bc2b-8e46ff2bcb31" saved
I0118 14:10:27.560769       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"data-my-rabbitmq-0", UID:"fb41eb6d-188d-4291-bc2b-8e46ff2bcb31", APIVersion:"v1", ResourceVersion:"14645", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-fb41eb6d-188d-4291-bc2b-8e46ff2bcb31
I0118 14:16:47.423080       1 controller.go:1332] provision "12000000-0000-0000-0000-000000000000/data-ibraheem102-guestbook-rabbitmq-0" class "standard": started
I0118 14:16:47.423254       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    eb1b9de1-4277-4e50-9c9e-757e99949424 269 0 2026-01-18 09:52:47 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2026-01-18 09:52:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-99fdd522-1667-4428-891f-d51de69740b0 &PersistentVolumeClaim{ObjectMeta:{data-ibraheem102-guestbook-rabbitmq-0  12000000-0000-0000-0000-000000000000  99fdd522-1667-4428-891f-d51de69740b0 15061 0 2026-01-18 14:16:47 +0000 UTC <nil> <nil> map[app.kubernetes.io/instance:ibraheem102-guestbook app.kubernetes.io/name:rabbitmq] map[volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2026-01-18 14:16:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}},"f:labels":{".":{},"f:app.kubernetes.io/instance":{},"f:app.kubernetes.io/name":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{8589934592 0} {<nil>}  BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/12000000-0000-0000-0000-000000000000/data-ibraheem102-guestbook-rabbitmq-0
I0118 14:16:47.424204       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"12000000-0000-0000-0000-000000000000", Name:"data-ibraheem102-guestbook-rabbitmq-0", UID:"99fdd522-1667-4428-891f-d51de69740b0", APIVersion:"v1", ResourceVersion:"15061", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "12000000-0000-0000-0000-000000000000/data-ibraheem102-guestbook-rabbitmq-0"
I0118 14:16:47.424596       1 controller.go:1439] provision "12000000-0000-0000-0000-000000000000/data-ibraheem102-guestbook-rabbitmq-0" class "standard": volume "pvc-99fdd522-1667-4428-891f-d51de69740b0" provisioned
I0118 14:16:47.424650       1 controller.go:1456] provision "12000000-0000-0000-0000-000000000000/data-ibraheem102-guestbook-rabbitmq-0" class "standard": succeeded
I0118 14:16:47.424668       1 volume_store.go:212] Trying to save persistentvolume "pvc-99fdd522-1667-4428-891f-d51de69740b0"
I0118 14:16:47.516400       1 volume_store.go:219] persistentvolume "pvc-99fdd522-1667-4428-891f-d51de69740b0" saved
I0118 14:16:47.516676       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"12000000-0000-0000-0000-000000000000", Name:"data-ibraheem102-guestbook-rabbitmq-0", UID:"99fdd522-1667-4428-891f-d51de69740b0", APIVersion:"v1", ResourceVersion:"15061", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-99fdd522-1667-4428-891f-d51de69740b0
I0118 14:24:54.938526       1 controller.go:1332] provision "12000000-0000-0000-0000-000000000000/data-ibraheem103-guestbook-rabbitmq-0" class "standard": started
I0118 14:24:54.938705       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    eb1b9de1-4277-4e50-9c9e-757e99949424 269 0 2026-01-18 09:52:47 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2026-01-18 09:52:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-62910b5a-7342-4978-85c9-1d69aaaa9474 &PersistentVolumeClaim{ObjectMeta:{data-ibraheem103-guestbook-rabbitmq-0  12000000-0000-0000-0000-000000000000  62910b5a-7342-4978-85c9-1d69aaaa9474 15687 0 2026-01-18 14:24:54 +0000 UTC <nil> <nil> map[app.kubernetes.io/instance:ibraheem103-guestbook app.kubernetes.io/name:rabbitmq] map[volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2026-01-18 14:24:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}},"f:labels":{".":{},"f:app.kubernetes.io/instance":{},"f:app.kubernetes.io/name":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{8589934592 0} {<nil>}  BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/12000000-0000-0000-0000-000000000000/data-ibraheem103-guestbook-rabbitmq-0
I0118 14:24:54.938992       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"12000000-0000-0000-0000-000000000000", Name:"data-ibraheem103-guestbook-rabbitmq-0", UID:"62910b5a-7342-4978-85c9-1d69aaaa9474", APIVersion:"v1", ResourceVersion:"15687", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "12000000-0000-0000-0000-000000000000/data-ibraheem103-guestbook-rabbitmq-0"
I0118 14:24:54.939409       1 controller.go:1439] provision "12000000-0000-0000-0000-000000000000/data-ibraheem103-guestbook-rabbitmq-0" class "standard": volume "pvc-62910b5a-7342-4978-85c9-1d69aaaa9474" provisioned
I0118 14:24:54.939443       1 controller.go:1456] provision "12000000-0000-0000-0000-000000000000/data-ibraheem103-guestbook-rabbitmq-0" class "standard": succeeded
I0118 14:24:54.939471       1 volume_store.go:212] Trying to save persistentvolume "pvc-62910b5a-7342-4978-85c9-1d69aaaa9474"
I0118 14:24:55.121079       1 volume_store.go:219] persistentvolume "pvc-62910b5a-7342-4978-85c9-1d69aaaa9474" saved
I0118 14:24:55.121714       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"12000000-0000-0000-0000-000000000000", Name:"data-ibraheem103-guestbook-rabbitmq-0", UID:"62910b5a-7342-4978-85c9-1d69aaaa9474", APIVersion:"v1", ResourceVersion:"15687", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-62910b5a-7342-4978-85c9-1d69aaaa9474


==> storage-provisioner [f7e383d6625e] <==
I0118 10:10:19.010322       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0118 10:10:41.235359       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

